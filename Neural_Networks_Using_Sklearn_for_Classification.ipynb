{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFopIDwcj4eR"
   },
   "source": [
    "# Libraries for Neural Networks - sklearn 1 (classification)\n",
    "\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1xgZhek0467AtlfupqvovcjoFIJ2dB4in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmwQCDnBmJil"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJM3q5bMjtBi"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seLQVWizktHd"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0Xt-g4ICk1Hn",
    "outputId": "b2c94fa7-781b-4e21-ada3-5710b157de4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = iris.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "eLqWkIrkk7La",
    "outputId": "98d9ad2e-bb87-4787-dc01-8d9547b83b44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "SUPFkDvok9-N",
    "outputId": "d08753be-29b1-4667-f39b-e882c6fb1582"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = iris.target\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0TFN814OlB03",
    "outputId": "d01e2558-0455-48df-c968-d7a56faca8fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Z40re0xClGbq",
    "outputId": "da21e31f-0972-49e6-a235-42dbe3396b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "unWADzCblIKJ",
    "outputId": "065b31df-b02d-4ea5-d401-d7b2a83aa176"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF29OkpYmMsV"
   },
   "source": [
    "## Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13aLh0tolo8B"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rUo21vlal6TW",
    "outputId": "2a20774f-797d-42e4-d898-3a6da11b5f8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tSmm6Pnjl8fc",
    "outputId": "29dfa229-f2f7-4b37-cb89-7a9cc207ac84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5_hgLkU1mEFB",
    "outputId": "3e2f0a57-ed88-40d1-adef-4937ddbdd69a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "SaK4mWf1mGOL",
    "outputId": "9ae6634c-4937-450e-e923-ba4eb3ab5f7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "73mE_zqirSMa",
    "outputId": "1f7b94ac-622f-4e03-eee9-c44177a103ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6. , 2.2, 4. , 1. ],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.9, 3.2, 4.8, 1.8]])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "DHlw1VSwrVbc",
    "outputId": "0a438522-242a-4e2f-f457-7003f1c480a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.4, 1.5, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [5.1, 3.8, 1.9, 0.4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "cg3-KBFirdOG",
    "outputId": "523cbf42-acbc-48f1-b7de-99be3d4c086f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "FmSFduvkrhQu",
    "outputId": "ed340e78-ab3f-4689-ef08-aae6e456488c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S5gJUwomQ3U"
   },
   "source": [
    "## Neural network (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6UVIgN-2lKt0",
    "outputId": "3b0bf27e-c05a-439c-f06c-4a04d7027fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12962981\n",
      "Iteration 2, loss = 1.12616273\n",
      "Iteration 3, loss = 1.12270354\n",
      "Iteration 4, loss = 1.12023464\n",
      "Iteration 5, loss = 1.11780497\n",
      "Iteration 6, loss = 1.11528575\n",
      "Iteration 7, loss = 1.11364242\n",
      "Iteration 8, loss = 1.11102683\n",
      "Iteration 9, loss = 1.10945699\n",
      "Iteration 10, loss = 1.10828933\n",
      "Iteration 11, loss = 1.10683621\n",
      "Iteration 12, loss = 1.10569191\n",
      "Iteration 13, loss = 1.10479699\n",
      "Iteration 14, loss = 1.10386798\n",
      "Iteration 15, loss = 1.10293651\n",
      "Iteration 16, loss = 1.10208632\n",
      "Iteration 17, loss = 1.10147221\n",
      "Iteration 18, loss = 1.10098559\n",
      "Iteration 19, loss = 1.10044121\n",
      "Iteration 20, loss = 1.09980212\n",
      "Iteration 21, loss = 1.09937341\n",
      "Iteration 22, loss = 1.09923725\n",
      "Iteration 23, loss = 1.09862021\n",
      "Iteration 24, loss = 1.09856566\n",
      "Iteration 25, loss = 1.09829208\n",
      "Iteration 26, loss = 1.09779187\n",
      "Iteration 27, loss = 1.09770373\n",
      "Iteration 28, loss = 1.09738958\n",
      "Iteration 29, loss = 1.09712138\n",
      "Iteration 30, loss = 1.09710316\n",
      "Iteration 31, loss = 1.09676360\n",
      "Iteration 32, loss = 1.09667004\n",
      "Iteration 33, loss = 1.09643035\n",
      "Iteration 34, loss = 1.09631963\n",
      "Iteration 35, loss = 1.09621163\n",
      "Iteration 36, loss = 1.09608904\n",
      "Iteration 37, loss = 1.09585223\n",
      "Iteration 38, loss = 1.09572453\n",
      "Iteration 39, loss = 1.09551204\n",
      "Iteration 40, loss = 1.09534394\n",
      "Iteration 41, loss = 1.09524728\n",
      "Iteration 42, loss = 1.09532722\n",
      "Iteration 43, loss = 1.09496836\n",
      "Iteration 44, loss = 1.09468755\n",
      "Iteration 45, loss = 1.09453801\n",
      "Iteration 46, loss = 1.09431176\n",
      "Iteration 47, loss = 1.09415512\n",
      "Iteration 48, loss = 1.09390073\n",
      "Iteration 49, loss = 1.09359034\n",
      "Iteration 50, loss = 1.09334012\n",
      "Iteration 51, loss = 1.09308681\n",
      "Iteration 52, loss = 1.09270751\n",
      "Iteration 53, loss = 1.09240000\n",
      "Iteration 54, loss = 1.09201929\n",
      "Iteration 55, loss = 1.09163128\n",
      "Iteration 56, loss = 1.09121151\n",
      "Iteration 57, loss = 1.09075146\n",
      "Iteration 58, loss = 1.09023598\n",
      "Iteration 59, loss = 1.08974775\n",
      "Iteration 60, loss = 1.08924632\n",
      "Iteration 61, loss = 1.08867283\n",
      "Iteration 62, loss = 1.08815282\n",
      "Iteration 63, loss = 1.08739630\n",
      "Iteration 64, loss = 1.08667233\n",
      "Iteration 65, loss = 1.08605333\n",
      "Iteration 66, loss = 1.08524429\n",
      "Iteration 67, loss = 1.08461092\n",
      "Iteration 68, loss = 1.08391714\n",
      "Iteration 69, loss = 1.08291299\n",
      "Iteration 70, loss = 1.08210136\n",
      "Iteration 71, loss = 1.08124337\n",
      "Iteration 72, loss = 1.08031469\n",
      "Iteration 73, loss = 1.07947522\n",
      "Iteration 74, loss = 1.07849108\n",
      "Iteration 75, loss = 1.07741275\n",
      "Iteration 76, loss = 1.07638174\n",
      "Iteration 77, loss = 1.07543143\n",
      "Iteration 78, loss = 1.07423086\n",
      "Iteration 79, loss = 1.07316838\n",
      "Iteration 80, loss = 1.07204906\n",
      "Iteration 81, loss = 1.07084252\n",
      "Iteration 82, loss = 1.06972728\n",
      "Iteration 83, loss = 1.06839493\n",
      "Iteration 84, loss = 1.06708690\n",
      "Iteration 85, loss = 1.06583356\n",
      "Iteration 86, loss = 1.06448201\n",
      "Iteration 87, loss = 1.06302088\n",
      "Iteration 88, loss = 1.06165095\n",
      "Iteration 89, loss = 1.06008211\n",
      "Iteration 90, loss = 1.05848631\n",
      "Iteration 91, loss = 1.05712110\n",
      "Iteration 92, loss = 1.05532577\n",
      "Iteration 93, loss = 1.05364813\n",
      "Iteration 94, loss = 1.05204041\n",
      "Iteration 95, loss = 1.05030165\n",
      "Iteration 96, loss = 1.04844146\n",
      "Iteration 97, loss = 1.04671242\n",
      "Iteration 98, loss = 1.04468840\n",
      "Iteration 99, loss = 1.04282783\n",
      "Iteration 100, loss = 1.04085598\n",
      "Iteration 101, loss = 1.03882779\n",
      "Iteration 102, loss = 1.03666065\n",
      "Iteration 103, loss = 1.03452536\n",
      "Iteration 104, loss = 1.03248681\n",
      "Iteration 105, loss = 1.03008533\n",
      "Iteration 106, loss = 1.02796533\n",
      "Iteration 107, loss = 1.02555099\n",
      "Iteration 108, loss = 1.02316255\n",
      "Iteration 109, loss = 1.02075943\n",
      "Iteration 110, loss = 1.01824269\n",
      "Iteration 111, loss = 1.01587618\n",
      "Iteration 112, loss = 1.01309910\n",
      "Iteration 113, loss = 1.01047927\n",
      "Iteration 114, loss = 1.00780787\n",
      "Iteration 115, loss = 1.00520863\n",
      "Iteration 116, loss = 1.00222011\n",
      "Iteration 117, loss = 0.99926812\n",
      "Iteration 118, loss = 0.99650779\n",
      "Iteration 119, loss = 0.99342295\n",
      "Iteration 120, loss = 0.99040104\n",
      "Iteration 121, loss = 0.98733782\n",
      "Iteration 122, loss = 0.98416184\n",
      "Iteration 123, loss = 0.98096808\n",
      "Iteration 124, loss = 0.97785693\n",
      "Iteration 125, loss = 0.97435763\n",
      "Iteration 126, loss = 0.97126679\n",
      "Iteration 127, loss = 0.96754751\n",
      "Iteration 128, loss = 0.96403687\n",
      "Iteration 129, loss = 0.96048651\n",
      "Iteration 130, loss = 0.95694891\n",
      "Iteration 131, loss = 0.95330102\n",
      "Iteration 132, loss = 0.94957354\n",
      "Iteration 133, loss = 0.94582050\n",
      "Iteration 134, loss = 0.94199605\n",
      "Iteration 135, loss = 0.93821307\n",
      "Iteration 136, loss = 0.93422162\n",
      "Iteration 137, loss = 0.93040372\n",
      "Iteration 138, loss = 0.92629146\n",
      "Iteration 139, loss = 0.92216221\n",
      "Iteration 140, loss = 0.91807806\n",
      "Iteration 141, loss = 0.91404124\n",
      "Iteration 142, loss = 0.90983671\n",
      "Iteration 143, loss = 0.90553849\n",
      "Iteration 144, loss = 0.90124640\n",
      "Iteration 145, loss = 0.89697457\n",
      "Iteration 146, loss = 0.89278982\n",
      "Iteration 147, loss = 0.88827465\n",
      "Iteration 148, loss = 0.88398674\n",
      "Iteration 149, loss = 0.87952069\n",
      "Iteration 150, loss = 0.87508137\n",
      "Iteration 151, loss = 0.87060001\n",
      "Iteration 152, loss = 0.86620081\n",
      "Iteration 153, loss = 0.86169145\n",
      "Iteration 154, loss = 0.85719845\n",
      "Iteration 155, loss = 0.85264296\n",
      "Iteration 156, loss = 0.84826513\n",
      "Iteration 157, loss = 0.84391914\n",
      "Iteration 158, loss = 0.83926008\n",
      "Iteration 159, loss = 0.83472762\n",
      "Iteration 160, loss = 0.83024225\n",
      "Iteration 161, loss = 0.82578738\n",
      "Iteration 162, loss = 0.82152649\n",
      "Iteration 163, loss = 0.81697495\n",
      "Iteration 164, loss = 0.81256397\n",
      "Iteration 165, loss = 0.80804243\n",
      "Iteration 166, loss = 0.80368395\n",
      "Iteration 167, loss = 0.79942205\n",
      "Iteration 168, loss = 0.79498218\n",
      "Iteration 169, loss = 0.79068769\n",
      "Iteration 170, loss = 0.78663944\n",
      "Iteration 171, loss = 0.78204767\n",
      "Iteration 172, loss = 0.77783921\n",
      "Iteration 173, loss = 0.77359170\n",
      "Iteration 174, loss = 0.76945595\n",
      "Iteration 175, loss = 0.76525302\n",
      "Iteration 176, loss = 0.76120671\n",
      "Iteration 177, loss = 0.75712089\n",
      "Iteration 178, loss = 0.75304665\n",
      "Iteration 179, loss = 0.74907714\n",
      "Iteration 180, loss = 0.74513265\n",
      "Iteration 181, loss = 0.74133265\n",
      "Iteration 182, loss = 0.73731929\n",
      "Iteration 183, loss = 0.73353864\n",
      "Iteration 184, loss = 0.72971455\n",
      "Iteration 185, loss = 0.72601306\n",
      "Iteration 186, loss = 0.72232007\n",
      "Iteration 187, loss = 0.71864303\n",
      "Iteration 188, loss = 0.71500984\n",
      "Iteration 189, loss = 0.71163665\n",
      "Iteration 190, loss = 0.70795868\n",
      "Iteration 191, loss = 0.70453753\n",
      "Iteration 192, loss = 0.70112125\n",
      "Iteration 193, loss = 0.69773712\n",
      "Iteration 194, loss = 0.69450915\n",
      "Iteration 195, loss = 0.69116384\n",
      "Iteration 196, loss = 0.68793963\n",
      "Iteration 197, loss = 0.68478107\n",
      "Iteration 198, loss = 0.68165811\n",
      "Iteration 199, loss = 0.67866994\n",
      "Iteration 200, loss = 0.67555397\n",
      "Iteration 201, loss = 0.67260570\n",
      "Iteration 202, loss = 0.66966857\n",
      "Iteration 203, loss = 0.66675291\n",
      "Iteration 204, loss = 0.66393015\n",
      "Iteration 205, loss = 0.66112779\n",
      "Iteration 206, loss = 0.65843724\n",
      "Iteration 207, loss = 0.65572470\n",
      "Iteration 208, loss = 0.65300561\n",
      "Iteration 209, loss = 0.65040339\n",
      "Iteration 210, loss = 0.64783142\n",
      "Iteration 211, loss = 0.64531018\n",
      "Iteration 212, loss = 0.64282412\n",
      "Iteration 213, loss = 0.64041608\n",
      "Iteration 214, loss = 0.63796330\n",
      "Iteration 215, loss = 0.63559696\n",
      "Iteration 216, loss = 0.63326837\n",
      "Iteration 217, loss = 0.63111046\n",
      "Iteration 218, loss = 0.62880439\n",
      "Iteration 219, loss = 0.62655316\n",
      "Iteration 220, loss = 0.62443603\n",
      "Iteration 221, loss = 0.62229980\n",
      "Iteration 222, loss = 0.62028338\n",
      "Iteration 223, loss = 0.61815773\n",
      "Iteration 224, loss = 0.61613553\n",
      "Iteration 225, loss = 0.61417183\n",
      "Iteration 226, loss = 0.61221755\n",
      "Iteration 227, loss = 0.61034506\n",
      "Iteration 228, loss = 0.60843378\n",
      "Iteration 229, loss = 0.60659582\n",
      "Iteration 230, loss = 0.60478840\n",
      "Iteration 231, loss = 0.60297411\n",
      "Iteration 232, loss = 0.60123623\n",
      "Iteration 233, loss = 0.59950678\n",
      "Iteration 234, loss = 0.59787685\n",
      "Iteration 235, loss = 0.59614190\n",
      "Iteration 236, loss = 0.59447422\n",
      "Iteration 237, loss = 0.59286442\n",
      "Iteration 238, loss = 0.59130550\n",
      "Iteration 239, loss = 0.58980170\n",
      "Iteration 240, loss = 0.58820819\n",
      "Iteration 241, loss = 0.58673049\n",
      "Iteration 242, loss = 0.58533266\n",
      "Iteration 243, loss = 0.58379946\n",
      "Iteration 244, loss = 0.58238085\n",
      "Iteration 245, loss = 0.58097748\n",
      "Iteration 246, loss = 0.57957387\n",
      "Iteration 247, loss = 0.57824498\n",
      "Iteration 248, loss = 0.57698713\n",
      "Iteration 249, loss = 0.57558888\n",
      "Iteration 250, loss = 0.57432560\n",
      "Iteration 251, loss = 0.57305920\n",
      "Iteration 252, loss = 0.57179227\n",
      "Iteration 253, loss = 0.57054404\n",
      "Iteration 254, loss = 0.56947310\n",
      "Iteration 255, loss = 0.56816340\n",
      "Iteration 256, loss = 0.56706714\n",
      "Iteration 257, loss = 0.56584343\n",
      "Iteration 258, loss = 0.56476078\n",
      "Iteration 259, loss = 0.56365076\n",
      "Iteration 260, loss = 0.56257165\n",
      "Iteration 261, loss = 0.56148425\n",
      "Iteration 262, loss = 0.56040674\n",
      "Iteration 263, loss = 0.55937542\n",
      "Iteration 264, loss = 0.55836631\n",
      "Iteration 265, loss = 0.55732477\n",
      "Iteration 266, loss = 0.55633168\n",
      "Iteration 267, loss = 0.55538117\n",
      "Iteration 268, loss = 0.55440986\n",
      "Iteration 269, loss = 0.55350210\n",
      "Iteration 270, loss = 0.55253521\n",
      "Iteration 271, loss = 0.55161839\n",
      "Iteration 272, loss = 0.55069856\n",
      "Iteration 273, loss = 0.54981943\n",
      "Iteration 274, loss = 0.54894669\n",
      "Iteration 275, loss = 0.54810766\n",
      "Iteration 276, loss = 0.54722525\n",
      "Iteration 277, loss = 0.54637915\n",
      "Iteration 278, loss = 0.54556118\n",
      "Iteration 279, loss = 0.54481830\n",
      "Iteration 280, loss = 0.54395322\n",
      "Iteration 281, loss = 0.54315564\n",
      "Iteration 282, loss = 0.54236111\n",
      "Iteration 283, loss = 0.54161769\n",
      "Iteration 284, loss = 0.54086265\n",
      "Iteration 285, loss = 0.54011240\n",
      "Iteration 286, loss = 0.53940138\n",
      "Iteration 287, loss = 0.53866043\n",
      "Iteration 288, loss = 0.53793496\n",
      "Iteration 289, loss = 0.53724720\n",
      "Iteration 290, loss = 0.53656482\n",
      "Iteration 291, loss = 0.53585690\n",
      "Iteration 292, loss = 0.53517723\n",
      "Iteration 293, loss = 0.53448058\n",
      "Iteration 294, loss = 0.53383265\n",
      "Iteration 295, loss = 0.53317954\n",
      "Iteration 296, loss = 0.53255803\n",
      "Iteration 297, loss = 0.53192803\n",
      "Iteration 298, loss = 0.53131618\n",
      "Iteration 299, loss = 0.53068392\n",
      "Iteration 300, loss = 0.53006341\n",
      "Iteration 301, loss = 0.52948533\n",
      "Iteration 302, loss = 0.52890797\n",
      "Iteration 303, loss = 0.52829354\n",
      "Iteration 304, loss = 0.52772951\n",
      "Iteration 305, loss = 0.52716884\n",
      "Iteration 306, loss = 0.52668902\n",
      "Iteration 307, loss = 0.52607305\n",
      "Iteration 308, loss = 0.52553962\n",
      "Iteration 309, loss = 0.52499679\n",
      "Iteration 310, loss = 0.52446084\n",
      "Iteration 311, loss = 0.52394051\n",
      "Iteration 312, loss = 0.52337160\n",
      "Iteration 313, loss = 0.52287887\n",
      "Iteration 314, loss = 0.52237801\n",
      "Iteration 315, loss = 0.52184645\n",
      "Iteration 316, loss = 0.52135537\n",
      "Iteration 317, loss = 0.52088031\n",
      "Iteration 318, loss = 0.52038070\n",
      "Iteration 319, loss = 0.51990852\n",
      "Iteration 320, loss = 0.51945436\n",
      "Iteration 321, loss = 0.51898268\n",
      "Iteration 322, loss = 0.51850637\n",
      "Iteration 323, loss = 0.51803815\n",
      "Iteration 324, loss = 0.51761243\n",
      "Iteration 325, loss = 0.51714332\n",
      "Iteration 326, loss = 0.51671348\n",
      "Iteration 327, loss = 0.51631530\n",
      "Iteration 328, loss = 0.51582679\n",
      "Iteration 329, loss = 0.51541031\n",
      "Iteration 330, loss = 0.51499830\n",
      "Iteration 331, loss = 0.51460534\n",
      "Iteration 332, loss = 0.51415744\n",
      "Iteration 333, loss = 0.51374781\n",
      "Iteration 334, loss = 0.51333604\n",
      "Iteration 335, loss = 0.51297818\n",
      "Iteration 336, loss = 0.51253774\n",
      "Iteration 337, loss = 0.51215636\n",
      "Iteration 338, loss = 0.51178230\n",
      "Iteration 339, loss = 0.51136299\n",
      "Iteration 340, loss = 0.51100234\n",
      "Iteration 341, loss = 0.51066110\n",
      "Iteration 342, loss = 0.51033011\n",
      "Iteration 343, loss = 0.50986464\n",
      "Iteration 344, loss = 0.50950483\n",
      "Iteration 345, loss = 0.50914960\n",
      "Iteration 346, loss = 0.50880280\n",
      "Iteration 347, loss = 0.50845831\n",
      "Iteration 348, loss = 0.50804878\n",
      "Iteration 349, loss = 0.50774437\n",
      "Iteration 350, loss = 0.50738920\n",
      "Iteration 351, loss = 0.50703805\n",
      "Iteration 352, loss = 0.50670363\n",
      "Iteration 353, loss = 0.50635955\n",
      "Iteration 354, loss = 0.50600683\n",
      "Iteration 355, loss = 0.50573196\n",
      "Iteration 356, loss = 0.50533887\n",
      "Iteration 357, loss = 0.50500683\n",
      "Iteration 358, loss = 0.50468647\n",
      "Iteration 359, loss = 0.50436733\n",
      "Iteration 360, loss = 0.50403747\n",
      "Iteration 361, loss = 0.50369506\n",
      "Iteration 362, loss = 0.50354096\n",
      "Iteration 363, loss = 0.50308180\n",
      "Iteration 364, loss = 0.50274884\n",
      "Iteration 365, loss = 0.50246205\n",
      "Iteration 366, loss = 0.50220866\n",
      "Iteration 367, loss = 0.50181919\n",
      "Iteration 368, loss = 0.50151519\n",
      "Iteration 369, loss = 0.50121772\n",
      "Iteration 370, loss = 0.50089627\n",
      "Iteration 371, loss = 0.50060491\n",
      "Iteration 372, loss = 0.50029834\n",
      "Iteration 373, loss = 0.49999849\n",
      "Iteration 374, loss = 0.49977896\n",
      "Iteration 375, loss = 0.49939738\n",
      "Iteration 376, loss = 0.49912492\n",
      "Iteration 377, loss = 0.49880882\n",
      "Iteration 378, loss = 0.49854064\n",
      "Iteration 379, loss = 0.49824877\n",
      "Iteration 380, loss = 0.49794908\n",
      "Iteration 381, loss = 0.49762243\n",
      "Iteration 382, loss = 0.49735590\n",
      "Iteration 383, loss = 0.49706880\n",
      "Iteration 384, loss = 0.49675138\n",
      "Iteration 385, loss = 0.49645944\n",
      "Iteration 386, loss = 0.49617287\n",
      "Iteration 387, loss = 0.49589370\n",
      "Iteration 388, loss = 0.49557025\n",
      "Iteration 389, loss = 0.49527424\n",
      "Iteration 390, loss = 0.49496997\n",
      "Iteration 391, loss = 0.49468233\n",
      "Iteration 392, loss = 0.49447421\n",
      "Iteration 393, loss = 0.49407305\n",
      "Iteration 394, loss = 0.49377631\n",
      "Iteration 395, loss = 0.49348585\n",
      "Iteration 396, loss = 0.49324952\n",
      "Iteration 397, loss = 0.49292823\n",
      "Iteration 398, loss = 0.49256589\n",
      "Iteration 399, loss = 0.49223841\n",
      "Iteration 400, loss = 0.49191992\n",
      "Iteration 401, loss = 0.49158815\n",
      "Iteration 402, loss = 0.49127335\n",
      "Iteration 403, loss = 0.49094120\n",
      "Iteration 404, loss = 0.49061137\n",
      "Iteration 405, loss = 0.49033574\n",
      "Iteration 406, loss = 0.48993763\n",
      "Iteration 407, loss = 0.48958910\n",
      "Iteration 408, loss = 0.48923294\n",
      "Iteration 409, loss = 0.48887808\n",
      "Iteration 410, loss = 0.48854573\n",
      "Iteration 411, loss = 0.48813456\n",
      "Iteration 412, loss = 0.48780284\n",
      "Iteration 413, loss = 0.48741064\n",
      "Iteration 414, loss = 0.48699219\n",
      "Iteration 415, loss = 0.48657859\n",
      "Iteration 416, loss = 0.48619158\n",
      "Iteration 417, loss = 0.48577309\n",
      "Iteration 418, loss = 0.48534885\n",
      "Iteration 419, loss = 0.48490985\n",
      "Iteration 420, loss = 0.48446044\n",
      "Iteration 421, loss = 0.48402784\n",
      "Iteration 422, loss = 0.48357602\n",
      "Iteration 423, loss = 0.48313519\n",
      "Iteration 424, loss = 0.48257774\n",
      "Iteration 425, loss = 0.48209504\n",
      "Iteration 426, loss = 0.48160280\n",
      "Iteration 427, loss = 0.48107211\n",
      "Iteration 428, loss = 0.48056379\n",
      "Iteration 429, loss = 0.48006365\n",
      "Iteration 430, loss = 0.47946055\n",
      "Iteration 431, loss = 0.47887692\n",
      "Iteration 432, loss = 0.47832692\n",
      "Iteration 433, loss = 0.47773203\n",
      "Iteration 434, loss = 0.47713926\n",
      "Iteration 435, loss = 0.47657120\n",
      "Iteration 436, loss = 0.47593649\n",
      "Iteration 437, loss = 0.47531664\n",
      "Iteration 438, loss = 0.47467577\n",
      "Iteration 439, loss = 0.47401798\n",
      "Iteration 440, loss = 0.47340701\n",
      "Iteration 441, loss = 0.47277485\n",
      "Iteration 442, loss = 0.47203184\n",
      "Iteration 443, loss = 0.47137379\n",
      "Iteration 444, loss = 0.47070270\n",
      "Iteration 445, loss = 0.46998634\n",
      "Iteration 446, loss = 0.46929750\n",
      "Iteration 447, loss = 0.46855690\n",
      "Iteration 448, loss = 0.46785098\n",
      "Iteration 449, loss = 0.46711557\n",
      "Iteration 450, loss = 0.46631323\n",
      "Iteration 451, loss = 0.46554305\n",
      "Iteration 452, loss = 0.46472671\n",
      "Iteration 453, loss = 0.46408286\n",
      "Iteration 454, loss = 0.46316421\n",
      "Iteration 455, loss = 0.46238867\n",
      "Iteration 456, loss = 0.46152066\n",
      "Iteration 457, loss = 0.46061715\n",
      "Iteration 458, loss = 0.45979937\n",
      "Iteration 459, loss = 0.45888675\n",
      "Iteration 460, loss = 0.45807072\n",
      "Iteration 461, loss = 0.45705490\n",
      "Iteration 462, loss = 0.45612809\n",
      "Iteration 463, loss = 0.45522132\n",
      "Iteration 464, loss = 0.45419249\n",
      "Iteration 465, loss = 0.45329784\n",
      "Iteration 466, loss = 0.45216972\n",
      "Iteration 467, loss = 0.45121611\n",
      "Iteration 468, loss = 0.45015948\n",
      "Iteration 469, loss = 0.44902896\n",
      "Iteration 470, loss = 0.44794854\n",
      "Iteration 471, loss = 0.44684339\n",
      "Iteration 472, loss = 0.44584749\n",
      "Iteration 473, loss = 0.44459678\n",
      "Iteration 474, loss = 0.44342547\n",
      "Iteration 475, loss = 0.44233466\n",
      "Iteration 476, loss = 0.44106986\n",
      "Iteration 477, loss = 0.43987011\n",
      "Iteration 478, loss = 0.43870694\n",
      "Iteration 479, loss = 0.43750414\n",
      "Iteration 480, loss = 0.43624429\n",
      "Iteration 481, loss = 0.43504379\n",
      "Iteration 482, loss = 0.43402382\n",
      "Iteration 483, loss = 0.43260711\n",
      "Iteration 484, loss = 0.43131288\n",
      "Iteration 485, loss = 0.43003725\n",
      "Iteration 486, loss = 0.42876755\n",
      "Iteration 487, loss = 0.42760323\n",
      "Iteration 488, loss = 0.42632456\n",
      "Iteration 489, loss = 0.42513402\n",
      "Iteration 490, loss = 0.42377376\n",
      "Iteration 491, loss = 0.42254916\n",
      "Iteration 492, loss = 0.42123258\n",
      "Iteration 493, loss = 0.42005458\n",
      "Iteration 494, loss = 0.41863689\n",
      "Iteration 495, loss = 0.41734017\n",
      "Iteration 496, loss = 0.41596421\n",
      "Iteration 497, loss = 0.41478373\n",
      "Iteration 498, loss = 0.41342878\n",
      "Iteration 499, loss = 0.41201529\n",
      "Iteration 500, loss = 0.41075898\n",
      "Iteration 501, loss = 0.40934013\n",
      "Iteration 502, loss = 0.40830295\n",
      "Iteration 503, loss = 0.40669050\n",
      "Iteration 504, loss = 0.40534997\n",
      "Iteration 505, loss = 0.40399749\n",
      "Iteration 506, loss = 0.40273811\n",
      "Iteration 507, loss = 0.40121677\n",
      "Iteration 508, loss = 0.39989372\n",
      "Iteration 509, loss = 0.39865068\n",
      "Iteration 510, loss = 0.39711608\n",
      "Iteration 511, loss = 0.39572914\n",
      "Iteration 512, loss = 0.39435028\n",
      "Iteration 513, loss = 0.39299272\n",
      "Iteration 514, loss = 0.39162082\n",
      "Iteration 515, loss = 0.39037036\n",
      "Iteration 516, loss = 0.38884022\n",
      "Iteration 517, loss = 0.38736098\n",
      "Iteration 518, loss = 0.38596012\n",
      "Iteration 519, loss = 0.38464872\n",
      "Iteration 520, loss = 0.38325483\n",
      "Iteration 521, loss = 0.38190964\n",
      "Iteration 522, loss = 0.38039124\n",
      "Iteration 523, loss = 0.37901931\n",
      "Iteration 524, loss = 0.37753670\n",
      "Iteration 525, loss = 0.37624535\n",
      "Iteration 526, loss = 0.37476927\n",
      "Iteration 527, loss = 0.37364833\n",
      "Iteration 528, loss = 0.37192277\n",
      "Iteration 529, loss = 0.37048221\n",
      "Iteration 530, loss = 0.36910491\n",
      "Iteration 531, loss = 0.36766735\n",
      "Iteration 532, loss = 0.36634890\n",
      "Iteration 533, loss = 0.36502432\n",
      "Iteration 534, loss = 0.36353539\n",
      "Iteration 535, loss = 0.36199686\n",
      "Iteration 536, loss = 0.36058925\n",
      "Iteration 537, loss = 0.35918189\n",
      "Iteration 538, loss = 0.35780324\n",
      "Iteration 539, loss = 0.35649096\n",
      "Iteration 540, loss = 0.35484149\n",
      "Iteration 541, loss = 0.35351649\n",
      "Iteration 542, loss = 0.35220724\n",
      "Iteration 543, loss = 0.35078891\n",
      "Iteration 544, loss = 0.34929049\n",
      "Iteration 545, loss = 0.34783589\n",
      "Iteration 546, loss = 0.34638047\n",
      "Iteration 547, loss = 0.34515520\n",
      "Iteration 548, loss = 0.34363919\n",
      "Iteration 549, loss = 0.34225507\n",
      "Iteration 550, loss = 0.34084524\n",
      "Iteration 551, loss = 0.33939326\n",
      "Iteration 552, loss = 0.33814383\n",
      "Iteration 553, loss = 0.33683975\n",
      "Iteration 554, loss = 0.33520475\n",
      "Iteration 555, loss = 0.33397309\n",
      "Iteration 556, loss = 0.33261849\n",
      "Iteration 557, loss = 0.33114662\n",
      "Iteration 558, loss = 0.32982235\n",
      "Iteration 559, loss = 0.32853537\n",
      "Iteration 560, loss = 0.32793699\n",
      "Iteration 561, loss = 0.32602212\n",
      "Iteration 562, loss = 0.32453812\n",
      "Iteration 563, loss = 0.32317554\n",
      "Iteration 564, loss = 0.32217484\n",
      "Iteration 565, loss = 0.32060870\n",
      "Iteration 566, loss = 0.31937200\n",
      "Iteration 567, loss = 0.31809231\n",
      "Iteration 568, loss = 0.31702264\n",
      "Iteration 569, loss = 0.31556415\n",
      "Iteration 570, loss = 0.31422837\n",
      "Iteration 571, loss = 0.31290701\n",
      "Iteration 572, loss = 0.31205764\n",
      "Iteration 573, loss = 0.31044614\n",
      "Iteration 574, loss = 0.30919978\n",
      "Iteration 575, loss = 0.30788536\n",
      "Iteration 576, loss = 0.30673470\n",
      "Iteration 577, loss = 0.30539795\n",
      "Iteration 578, loss = 0.30425373\n",
      "Iteration 579, loss = 0.30310554\n",
      "Iteration 580, loss = 0.30204126\n",
      "Iteration 581, loss = 0.30079713\n",
      "Iteration 582, loss = 0.29942562\n",
      "Iteration 583, loss = 0.29825111\n",
      "Iteration 584, loss = 0.29697533\n",
      "Iteration 585, loss = 0.29597680\n",
      "Iteration 586, loss = 0.29502776\n",
      "Iteration 587, loss = 0.29344889\n",
      "Iteration 588, loss = 0.29230743\n",
      "Iteration 589, loss = 0.29113279\n",
      "Iteration 590, loss = 0.28992724\n",
      "Iteration 591, loss = 0.28884836\n",
      "Iteration 592, loss = 0.28768016\n",
      "Iteration 593, loss = 0.28685308\n",
      "Iteration 594, loss = 0.28549092\n",
      "Iteration 595, loss = 0.28436155\n",
      "Iteration 596, loss = 0.28333496\n",
      "Iteration 597, loss = 0.28201421\n",
      "Iteration 598, loss = 0.28089639\n",
      "Iteration 599, loss = 0.27985509\n",
      "Iteration 600, loss = 0.27880077\n",
      "Iteration 601, loss = 0.27769350\n",
      "Iteration 602, loss = 0.27687951\n",
      "Iteration 603, loss = 0.27552284\n",
      "Iteration 604, loss = 0.27445119\n",
      "Iteration 605, loss = 0.27332646\n",
      "Iteration 606, loss = 0.27225023\n",
      "Iteration 607, loss = 0.27116210\n",
      "Iteration 608, loss = 0.27012340\n",
      "Iteration 609, loss = 0.26919839\n",
      "Iteration 610, loss = 0.26804216\n",
      "Iteration 611, loss = 0.26710182\n",
      "Iteration 612, loss = 0.26612062\n",
      "Iteration 613, loss = 0.26497698\n",
      "Iteration 614, loss = 0.26411392\n",
      "Iteration 615, loss = 0.26297302\n",
      "Iteration 616, loss = 0.26193400\n",
      "Iteration 617, loss = 0.26092495\n",
      "Iteration 618, loss = 0.26045144\n",
      "Iteration 619, loss = 0.25905490\n",
      "Iteration 620, loss = 0.25802264\n",
      "Iteration 621, loss = 0.25696865\n",
      "Iteration 622, loss = 0.25599748\n",
      "Iteration 623, loss = 0.25519553\n",
      "Iteration 624, loss = 0.25416146\n",
      "Iteration 625, loss = 0.25319445\n",
      "Iteration 626, loss = 0.25229197\n",
      "Iteration 627, loss = 0.25134361\n",
      "Iteration 628, loss = 0.25074845\n",
      "Iteration 629, loss = 0.24922088\n",
      "Iteration 630, loss = 0.24843259\n",
      "Iteration 631, loss = 0.24753039\n",
      "Iteration 632, loss = 0.24645914\n",
      "Iteration 633, loss = 0.24557140\n",
      "Iteration 634, loss = 0.24472241\n",
      "Iteration 635, loss = 0.24374046\n",
      "Iteration 636, loss = 0.24325791\n",
      "Iteration 637, loss = 0.24204624\n",
      "Iteration 638, loss = 0.24114361\n",
      "Iteration 639, loss = 0.24021116\n",
      "Iteration 640, loss = 0.23956716\n",
      "Iteration 641, loss = 0.23839619\n",
      "Iteration 642, loss = 0.23770272\n",
      "Iteration 643, loss = 0.23684311\n",
      "Iteration 644, loss = 0.23597703\n",
      "Iteration 645, loss = 0.23509060\n",
      "Iteration 646, loss = 0.23412537\n",
      "Iteration 647, loss = 0.23322202\n",
      "Iteration 648, loss = 0.23238582\n",
      "Iteration 649, loss = 0.23152473\n",
      "Iteration 650, loss = 0.23090711\n",
      "Iteration 651, loss = 0.22997706\n",
      "Iteration 652, loss = 0.22909716\n",
      "Iteration 653, loss = 0.22816514\n",
      "Iteration 654, loss = 0.22745063\n",
      "Iteration 655, loss = 0.22663725\n",
      "Iteration 656, loss = 0.22582161\n",
      "Iteration 657, loss = 0.22505619\n",
      "Iteration 658, loss = 0.22424130\n",
      "Iteration 659, loss = 0.22338372\n",
      "Iteration 660, loss = 0.22256865\n",
      "Iteration 661, loss = 0.22190419\n",
      "Iteration 662, loss = 0.22107266\n",
      "Iteration 663, loss = 0.22023472\n",
      "Iteration 664, loss = 0.21956140\n",
      "Iteration 665, loss = 0.21879118\n",
      "Iteration 666, loss = 0.21796741\n",
      "Iteration 667, loss = 0.21717441\n",
      "Iteration 668, loss = 0.21642033\n",
      "Iteration 669, loss = 0.21567664\n",
      "Iteration 670, loss = 0.21484337\n",
      "Iteration 671, loss = 0.21410044\n",
      "Iteration 672, loss = 0.21338643\n",
      "Iteration 673, loss = 0.21268421\n",
      "Iteration 674, loss = 0.21195442\n",
      "Iteration 675, loss = 0.21151026\n",
      "Iteration 676, loss = 0.21042413\n",
      "Iteration 677, loss = 0.20970184\n",
      "Iteration 678, loss = 0.20897474\n",
      "Iteration 679, loss = 0.20828081\n",
      "Iteration 680, loss = 0.20768780\n",
      "Iteration 681, loss = 0.20693146\n",
      "Iteration 682, loss = 0.20622289\n",
      "Iteration 683, loss = 0.20555398\n",
      "Iteration 684, loss = 0.20495036\n",
      "Iteration 685, loss = 0.20416732\n",
      "Iteration 686, loss = 0.20352240\n",
      "Iteration 687, loss = 0.20269744\n",
      "Iteration 688, loss = 0.20204010\n",
      "Iteration 689, loss = 0.20129189\n",
      "Iteration 690, loss = 0.20061995\n",
      "Iteration 691, loss = 0.20002824\n",
      "Iteration 692, loss = 0.19932569\n",
      "Iteration 693, loss = 0.19871450\n",
      "Iteration 694, loss = 0.19824035\n",
      "Iteration 695, loss = 0.19739329\n",
      "Iteration 696, loss = 0.19694601\n",
      "Iteration 697, loss = 0.19606303\n",
      "Iteration 698, loss = 0.19537470\n",
      "Iteration 699, loss = 0.19477143\n",
      "Iteration 700, loss = 0.19473378\n",
      "Iteration 701, loss = 0.19355219\n",
      "Iteration 702, loss = 0.19279627\n",
      "Iteration 703, loss = 0.19233392\n",
      "Iteration 704, loss = 0.19164445\n",
      "Iteration 705, loss = 0.19101149\n",
      "Iteration 706, loss = 0.19046110\n",
      "Iteration 707, loss = 0.18988087\n",
      "Iteration 708, loss = 0.18916620\n",
      "Iteration 709, loss = 0.18856512\n",
      "Iteration 710, loss = 0.18795899\n",
      "Iteration 711, loss = 0.18731035\n",
      "Iteration 712, loss = 0.18712054\n",
      "Iteration 713, loss = 0.18622219\n",
      "Iteration 714, loss = 0.18547082\n",
      "Iteration 715, loss = 0.18487661\n",
      "Iteration 716, loss = 0.18439226\n",
      "Iteration 717, loss = 0.18392126\n",
      "Iteration 718, loss = 0.18346514\n",
      "Iteration 719, loss = 0.18279994\n",
      "Iteration 720, loss = 0.18206773\n",
      "Iteration 721, loss = 0.18176809\n",
      "Iteration 722, loss = 0.18111289\n",
      "Iteration 723, loss = 0.18039675\n",
      "Iteration 724, loss = 0.17981669\n",
      "Iteration 725, loss = 0.17930019\n",
      "Iteration 726, loss = 0.17883571\n",
      "Iteration 727, loss = 0.17815080\n",
      "Iteration 728, loss = 0.17787882\n",
      "Iteration 729, loss = 0.17710956\n",
      "Iteration 730, loss = 0.17656180\n",
      "Iteration 731, loss = 0.17615285\n",
      "Iteration 732, loss = 0.17557418\n",
      "Iteration 733, loss = 0.17517100\n",
      "Iteration 734, loss = 0.17438735\n",
      "Iteration 735, loss = 0.17398541\n",
      "Iteration 736, loss = 0.17340556\n",
      "Iteration 737, loss = 0.17282682\n",
      "Iteration 738, loss = 0.17253109\n",
      "Iteration 739, loss = 0.17205476\n",
      "Iteration 740, loss = 0.17132215\n",
      "Iteration 741, loss = 0.17084437\n",
      "Iteration 742, loss = 0.17053952\n",
      "Iteration 743, loss = 0.16984172\n",
      "Iteration 744, loss = 0.16928116\n",
      "Iteration 745, loss = 0.16886225\n",
      "Iteration 746, loss = 0.16838445\n",
      "Iteration 747, loss = 0.16789826\n",
      "Iteration 748, loss = 0.16772783\n",
      "Iteration 749, loss = 0.16720065\n",
      "Iteration 750, loss = 0.16631600\n",
      "Iteration 751, loss = 0.16642172\n",
      "Iteration 752, loss = 0.16561109\n",
      "Iteration 753, loss = 0.16503314\n",
      "Iteration 754, loss = 0.16461756\n",
      "Iteration 755, loss = 0.16503058\n",
      "Iteration 756, loss = 0.16351724\n",
      "Iteration 757, loss = 0.16309044\n",
      "Iteration 758, loss = 0.16265125\n",
      "Iteration 759, loss = 0.16212706\n",
      "Iteration 760, loss = 0.16162475\n",
      "Iteration 761, loss = 0.16123428\n",
      "Iteration 762, loss = 0.16083960\n",
      "Iteration 763, loss = 0.16027712\n",
      "Iteration 764, loss = 0.15987312\n",
      "Iteration 765, loss = 0.15943414\n",
      "Iteration 766, loss = 0.15916310\n",
      "Iteration 767, loss = 0.15871860\n",
      "Iteration 768, loss = 0.15826169\n",
      "Iteration 769, loss = 0.15765567\n",
      "Iteration 770, loss = 0.15724281\n",
      "Iteration 771, loss = 0.15691078\n",
      "Iteration 772, loss = 0.15644165\n",
      "Iteration 773, loss = 0.15597380\n",
      "Iteration 774, loss = 0.15571437\n",
      "Iteration 775, loss = 0.15495727\n",
      "Iteration 776, loss = 0.15475241\n",
      "Iteration 777, loss = 0.15424740\n",
      "Iteration 778, loss = 0.15385694\n",
      "Iteration 779, loss = 0.15378624\n",
      "Iteration 780, loss = 0.15307542\n",
      "Iteration 781, loss = 0.15267392\n",
      "Iteration 782, loss = 0.15218074\n",
      "Iteration 783, loss = 0.15184272\n",
      "Iteration 784, loss = 0.15134525\n",
      "Iteration 785, loss = 0.15107427\n",
      "Iteration 786, loss = 0.15068977\n",
      "Iteration 787, loss = 0.15022688\n",
      "Iteration 788, loss = 0.14991791\n",
      "Iteration 789, loss = 0.14936423\n",
      "Iteration 790, loss = 0.14911543\n",
      "Iteration 791, loss = 0.14870774\n",
      "Iteration 792, loss = 0.14822373\n",
      "Iteration 793, loss = 0.14831742\n",
      "Iteration 794, loss = 0.14759357\n",
      "Iteration 795, loss = 0.14706913\n",
      "Iteration 796, loss = 0.14678971\n",
      "Iteration 797, loss = 0.14628311\n",
      "Iteration 798, loss = 0.14596443\n",
      "Iteration 799, loss = 0.14574850\n",
      "Iteration 800, loss = 0.14514618\n",
      "Iteration 801, loss = 0.14486505\n",
      "Iteration 802, loss = 0.14484643\n",
      "Iteration 803, loss = 0.14410186\n",
      "Iteration 804, loss = 0.14374471\n",
      "Iteration 805, loss = 0.14365259\n",
      "Iteration 806, loss = 0.14301845\n",
      "Iteration 807, loss = 0.14290571\n",
      "Iteration 808, loss = 0.14229880\n",
      "Iteration 809, loss = 0.14191650\n",
      "Iteration 810, loss = 0.14154435\n",
      "Iteration 811, loss = 0.14128746\n",
      "Iteration 812, loss = 0.14113296\n",
      "Iteration 813, loss = 0.14061106\n",
      "Iteration 814, loss = 0.14041293\n",
      "Iteration 815, loss = 0.13993630\n",
      "Iteration 816, loss = 0.13989072\n",
      "Iteration 817, loss = 0.13928355\n",
      "Iteration 818, loss = 0.13886015\n",
      "Iteration 819, loss = 0.13854534\n",
      "Iteration 820, loss = 0.13820813\n",
      "Iteration 821, loss = 0.13791036\n",
      "Iteration 822, loss = 0.13753509\n",
      "Iteration 823, loss = 0.13723533\n",
      "Iteration 824, loss = 0.13694393\n",
      "Iteration 825, loss = 0.13659334\n",
      "Iteration 826, loss = 0.13623566\n",
      "Iteration 827, loss = 0.13592016\n",
      "Iteration 828, loss = 0.13566267\n",
      "Iteration 829, loss = 0.13539614\n",
      "Iteration 830, loss = 0.13493309\n",
      "Iteration 831, loss = 0.13471042\n",
      "Iteration 832, loss = 0.13422088\n",
      "Iteration 833, loss = 0.13400930\n",
      "Iteration 834, loss = 0.13367704\n",
      "Iteration 835, loss = 0.13334692\n",
      "Iteration 836, loss = 0.13316042\n",
      "Iteration 837, loss = 0.13293427\n",
      "Iteration 838, loss = 0.13243081\n",
      "Iteration 839, loss = 0.13236072\n",
      "Iteration 840, loss = 0.13191306\n",
      "Iteration 841, loss = 0.13244741\n",
      "Iteration 842, loss = 0.13131870\n",
      "Iteration 843, loss = 0.13099739\n",
      "Iteration 844, loss = 0.13055833\n",
      "Iteration 845, loss = 0.13029270\n",
      "Iteration 846, loss = 0.13010446\n",
      "Iteration 847, loss = 0.12989481\n",
      "Iteration 848, loss = 0.12948073\n",
      "Iteration 849, loss = 0.12922495\n",
      "Iteration 850, loss = 0.12885367\n",
      "Iteration 851, loss = 0.12847869\n",
      "Iteration 852, loss = 0.12829962\n",
      "Iteration 853, loss = 0.12799782\n",
      "Iteration 854, loss = 0.12784974\n",
      "Iteration 855, loss = 0.12754112\n",
      "Iteration 856, loss = 0.12715234\n",
      "Iteration 857, loss = 0.12691418\n",
      "Iteration 858, loss = 0.12660830\n",
      "Iteration 859, loss = 0.12630977\n",
      "Iteration 860, loss = 0.12624211\n",
      "Iteration 861, loss = 0.12569973\n",
      "Iteration 862, loss = 0.12548531\n",
      "Iteration 863, loss = 0.12517014\n",
      "Iteration 864, loss = 0.12521471\n",
      "Iteration 865, loss = 0.12473362\n",
      "Iteration 866, loss = 0.12445357\n",
      "Iteration 867, loss = 0.12424548\n",
      "Iteration 868, loss = 0.12401421\n",
      "Iteration 869, loss = 0.12408842\n",
      "Iteration 870, loss = 0.12350864\n",
      "Iteration 871, loss = 0.12302158\n",
      "Iteration 872, loss = 0.12290597\n",
      "Iteration 873, loss = 0.12267169\n",
      "Iteration 874, loss = 0.12238949\n",
      "Iteration 875, loss = 0.12203762\n",
      "Iteration 876, loss = 0.12173766\n",
      "Iteration 877, loss = 0.12151958\n",
      "Iteration 878, loss = 0.12123407\n",
      "Iteration 879, loss = 0.12105149\n",
      "Iteration 880, loss = 0.12068307\n",
      "Iteration 881, loss = 0.12108551\n",
      "Iteration 882, loss = 0.12045676\n",
      "Iteration 883, loss = 0.12013649\n",
      "Iteration 884, loss = 0.11977862\n",
      "Iteration 885, loss = 0.11970389\n",
      "Iteration 886, loss = 0.11933181\n",
      "Iteration 887, loss = 0.11895940\n",
      "Iteration 888, loss = 0.11887762\n",
      "Iteration 889, loss = 0.11855444\n",
      "Iteration 890, loss = 0.11849719\n",
      "Iteration 891, loss = 0.11805362\n",
      "Iteration 892, loss = 0.11793267\n",
      "Iteration 893, loss = 0.11767086\n",
      "Iteration 894, loss = 0.11730745\n",
      "Iteration 895, loss = 0.11710445\n",
      "Iteration 896, loss = 0.11697589\n",
      "Iteration 897, loss = 0.11657159\n",
      "Iteration 898, loss = 0.11641024\n",
      "Iteration 899, loss = 0.11621668\n",
      "Iteration 900, loss = 0.11625629\n",
      "Iteration 901, loss = 0.11584541\n",
      "Iteration 902, loss = 0.11544051\n",
      "Iteration 903, loss = 0.11528264\n",
      "Iteration 904, loss = 0.11499064\n",
      "Iteration 905, loss = 0.11489052\n",
      "Iteration 906, loss = 0.11452168\n",
      "Iteration 907, loss = 0.11431894\n",
      "Iteration 908, loss = 0.11413138\n",
      "Iteration 909, loss = 0.11396839\n",
      "Iteration 910, loss = 0.11363974\n",
      "Iteration 911, loss = 0.11341497\n",
      "Iteration 912, loss = 0.11328141\n",
      "Iteration 913, loss = 0.11300088\n",
      "Iteration 914, loss = 0.11302898\n",
      "Iteration 915, loss = 0.11260982\n",
      "Iteration 916, loss = 0.11251757\n",
      "Iteration 917, loss = 0.11214137\n",
      "Iteration 918, loss = 0.11196021\n",
      "Iteration 919, loss = 0.11174055\n",
      "Iteration 920, loss = 0.11153190\n",
      "Iteration 921, loss = 0.11136908\n",
      "Iteration 922, loss = 0.11117255\n",
      "Iteration 923, loss = 0.11098814\n",
      "Iteration 924, loss = 0.11082081\n",
      "Iteration 925, loss = 0.11053869\n",
      "Iteration 926, loss = 0.11028847\n",
      "Iteration 927, loss = 0.11007207\n",
      "Iteration 928, loss = 0.11005639\n",
      "Iteration 929, loss = 0.10968829\n",
      "Iteration 930, loss = 0.10956349\n",
      "Iteration 931, loss = 0.10921158\n",
      "Iteration 932, loss = 0.10914239\n",
      "Iteration 933, loss = 0.10881599\n",
      "Iteration 934, loss = 0.10888591\n",
      "Iteration 935, loss = 0.10847808\n",
      "Iteration 936, loss = 0.10839704\n",
      "Iteration 937, loss = 0.10819517\n",
      "Iteration 938, loss = 0.10790089\n",
      "Iteration 939, loss = 0.10763549\n",
      "Iteration 940, loss = 0.10750383\n",
      "Iteration 941, loss = 0.10720782\n",
      "Iteration 942, loss = 0.10713096\n",
      "Iteration 943, loss = 0.10696110\n",
      "Iteration 944, loss = 0.10682310\n",
      "Iteration 945, loss = 0.10662265\n",
      "Iteration 946, loss = 0.10635822\n",
      "Iteration 947, loss = 0.10610949\n",
      "Iteration 948, loss = 0.10607904\n",
      "Iteration 949, loss = 0.10585564\n",
      "Iteration 950, loss = 0.10563768\n",
      "Iteration 951, loss = 0.10544396\n",
      "Iteration 952, loss = 0.10549826\n",
      "Iteration 953, loss = 0.10519111\n",
      "Iteration 954, loss = 0.10496381\n",
      "Iteration 955, loss = 0.10469791\n",
      "Iteration 956, loss = 0.10457348\n",
      "Iteration 957, loss = 0.10431662\n",
      "Iteration 958, loss = 0.10417905\n",
      "Iteration 959, loss = 0.10389009\n",
      "Iteration 960, loss = 0.10377315\n",
      "Iteration 961, loss = 0.10356708\n",
      "Iteration 962, loss = 0.10342339\n",
      "Iteration 963, loss = 0.10321997\n",
      "Iteration 964, loss = 0.10303173\n",
      "Iteration 965, loss = 0.10288701\n",
      "Iteration 966, loss = 0.10268839\n",
      "Iteration 967, loss = 0.10253651\n",
      "Iteration 968, loss = 0.10256774\n",
      "Iteration 969, loss = 0.10256107\n",
      "Iteration 970, loss = 0.10205924\n",
      "Iteration 971, loss = 0.10205198\n",
      "Iteration 972, loss = 0.10179310\n",
      "Iteration 973, loss = 0.10207143\n",
      "Iteration 974, loss = 0.10134597\n",
      "Iteration 975, loss = 0.10117684\n",
      "Iteration 976, loss = 0.10097419\n",
      "Iteration 977, loss = 0.10083760\n",
      "Iteration 978, loss = 0.10073503\n",
      "Iteration 979, loss = 0.10055241\n",
      "Iteration 980, loss = 0.10041112\n",
      "Iteration 981, loss = 0.10019437\n",
      "Iteration 982, loss = 0.10011703\n",
      "Iteration 983, loss = 0.09989760\n",
      "Iteration 984, loss = 0.09972823\n",
      "Iteration 985, loss = 0.09992267\n",
      "Iteration 986, loss = 0.09960702\n",
      "Iteration 987, loss = 0.09911261\n",
      "Iteration 988, loss = 0.09901615\n",
      "Iteration 989, loss = 0.09901485\n",
      "Iteration 990, loss = 0.09874475\n",
      "Iteration 991, loss = 0.09860470\n",
      "Iteration 992, loss = 0.09836879\n",
      "Iteration 993, loss = 0.09826426\n",
      "Iteration 994, loss = 0.09805383\n",
      "Iteration 995, loss = 0.09801955\n",
      "Iteration 996, loss = 0.09773837\n",
      "Iteration 997, loss = 0.09756988\n",
      "Iteration 998, loss = 0.09761499\n",
      "Iteration 999, loss = 0.09735833\n",
      "Iteration 1000, loss = 0.09712722\n",
      "Iteration 1001, loss = 0.09734201\n",
      "Iteration 1002, loss = 0.09692018\n",
      "Iteration 1003, loss = 0.09679565\n",
      "Iteration 1004, loss = 0.09651529\n",
      "Iteration 1005, loss = 0.09640933\n",
      "Iteration 1006, loss = 0.09631721\n",
      "Iteration 1007, loss = 0.09617456\n",
      "Iteration 1008, loss = 0.09608883\n",
      "Iteration 1009, loss = 0.09606077\n",
      "Iteration 1010, loss = 0.09566757\n",
      "Iteration 1011, loss = 0.09543644\n",
      "Iteration 1012, loss = 0.09554406\n",
      "Iteration 1013, loss = 0.09537265\n",
      "Iteration 1014, loss = 0.09511489\n",
      "Iteration 1015, loss = 0.09494429\n",
      "Iteration 1016, loss = 0.09474923\n",
      "Iteration 1017, loss = 0.09472675\n",
      "Iteration 1018, loss = 0.09450731\n",
      "Iteration 1019, loss = 0.09455073\n",
      "Iteration 1020, loss = 0.09443017\n",
      "Iteration 1021, loss = 0.09403034\n",
      "Iteration 1022, loss = 0.09404982\n",
      "Iteration 1023, loss = 0.09377865\n",
      "Iteration 1024, loss = 0.09367855\n",
      "Iteration 1025, loss = 0.09347837\n",
      "Iteration 1026, loss = 0.09333792\n",
      "Iteration 1027, loss = 0.09327392\n",
      "Iteration 1028, loss = 0.09303088\n",
      "Iteration 1029, loss = 0.09316776\n",
      "Iteration 1030, loss = 0.09279958\n",
      "Iteration 1031, loss = 0.09291306\n",
      "Iteration 1032, loss = 0.09300872\n",
      "Iteration 1033, loss = 0.09268177\n",
      "Iteration 1034, loss = 0.09238443\n",
      "Iteration 1035, loss = 0.09223964\n",
      "Iteration 1036, loss = 0.09209923\n",
      "Iteration 1037, loss = 0.09184865\n",
      "Iteration 1038, loss = 0.09176243\n",
      "Iteration 1039, loss = 0.09162514\n",
      "Iteration 1040, loss = 0.09157917\n",
      "Iteration 1041, loss = 0.09130384\n",
      "Iteration 1042, loss = 0.09122121\n",
      "Iteration 1043, loss = 0.09123196\n",
      "Iteration 1044, loss = 0.09101239\n",
      "Iteration 1045, loss = 0.09081582\n",
      "Iteration 1046, loss = 0.09069130\n",
      "Iteration 1047, loss = 0.09060133\n",
      "Iteration 1048, loss = 0.09049894\n",
      "Iteration 1049, loss = 0.09030279\n",
      "Iteration 1050, loss = 0.09024128\n",
      "Iteration 1051, loss = 0.09013521\n",
      "Iteration 1052, loss = 0.08989421\n",
      "Iteration 1053, loss = 0.08983840\n",
      "Iteration 1054, loss = 0.08962308\n",
      "Iteration 1055, loss = 0.08949725\n",
      "Iteration 1056, loss = 0.08958981\n",
      "Iteration 1057, loss = 0.08976136\n",
      "Iteration 1058, loss = 0.08919083\n",
      "Iteration 1059, loss = 0.08916857\n",
      "Iteration 1060, loss = 0.08916955\n",
      "Iteration 1061, loss = 0.08879016\n",
      "Iteration 1062, loss = 0.08864856\n",
      "Iteration 1063, loss = 0.08853486\n",
      "Iteration 1064, loss = 0.08864137\n",
      "Iteration 1065, loss = 0.08864071\n",
      "Iteration 1066, loss = 0.08821677\n",
      "Iteration 1067, loss = 0.08806951\n",
      "Iteration 1068, loss = 0.08863613\n",
      "Iteration 1069, loss = 0.08795826\n",
      "Iteration 1070, loss = 0.08801312\n",
      "Iteration 1071, loss = 0.08784330\n",
      "Iteration 1072, loss = 0.08753312\n",
      "Iteration 1073, loss = 0.08774940\n",
      "Iteration 1074, loss = 0.08729621\n",
      "Iteration 1075, loss = 0.08708503\n",
      "Iteration 1076, loss = 0.08713941\n",
      "Iteration 1077, loss = 0.08690240\n",
      "Iteration 1078, loss = 0.08669343\n",
      "Iteration 1079, loss = 0.08669689\n",
      "Iteration 1080, loss = 0.08686407\n",
      "Iteration 1081, loss = 0.08663505\n",
      "Iteration 1082, loss = 0.08644557\n",
      "Iteration 1083, loss = 0.08648442\n",
      "Iteration 1084, loss = 0.08612781\n",
      "Iteration 1085, loss = 0.08607304\n",
      "Iteration 1086, loss = 0.08591825\n",
      "Iteration 1087, loss = 0.08598383\n",
      "Iteration 1088, loss = 0.08559723\n",
      "Iteration 1089, loss = 0.08580490\n",
      "Iteration 1090, loss = 0.08545536\n",
      "Iteration 1091, loss = 0.08545050\n",
      "Iteration 1092, loss = 0.08529185\n",
      "Iteration 1093, loss = 0.08514615\n",
      "Iteration 1094, loss = 0.08496445\n",
      "Iteration 1095, loss = 0.08500370\n",
      "Iteration 1096, loss = 0.08476503\n",
      "Iteration 1097, loss = 0.08504477\n",
      "Iteration 1098, loss = 0.08456854\n",
      "Iteration 1099, loss = 0.08452582\n",
      "Iteration 1100, loss = 0.08443553\n",
      "Iteration 1101, loss = 0.08449745\n",
      "Iteration 1102, loss = 0.08426168\n",
      "Iteration 1103, loss = 0.08407977\n",
      "Iteration 1104, loss = 0.08408025\n",
      "Iteration 1105, loss = 0.08386213\n",
      "Iteration 1106, loss = 0.08379278\n",
      "Iteration 1107, loss = 0.08361695\n",
      "Iteration 1108, loss = 0.08427107\n",
      "Iteration 1109, loss = 0.08360564\n",
      "Iteration 1110, loss = 0.08352441\n",
      "Iteration 1111, loss = 0.08319154\n",
      "Iteration 1112, loss = 0.08311700\n",
      "Iteration 1113, loss = 0.08312018\n",
      "Iteration 1114, loss = 0.08302720\n",
      "Iteration 1115, loss = 0.08277374\n",
      "Iteration 1116, loss = 0.08267593\n",
      "Iteration 1117, loss = 0.08260009\n",
      "Iteration 1118, loss = 0.08251949\n",
      "Iteration 1119, loss = 0.08262116\n",
      "Iteration 1120, loss = 0.08233692\n",
      "Iteration 1121, loss = 0.08233901\n",
      "Iteration 1122, loss = 0.08221471\n",
      "Iteration 1123, loss = 0.08214840\n",
      "Iteration 1124, loss = 0.08199939\n",
      "Iteration 1125, loss = 0.08192961\n",
      "Iteration 1126, loss = 0.08170243\n",
      "Iteration 1127, loss = 0.08156788\n",
      "Iteration 1128, loss = 0.08151439\n",
      "Iteration 1129, loss = 0.08148872\n",
      "Iteration 1130, loss = 0.08190916\n",
      "Iteration 1131, loss = 0.08126025\n",
      "Iteration 1132, loss = 0.08138438\n",
      "Iteration 1133, loss = 0.08108850\n",
      "Iteration 1134, loss = 0.08109054\n",
      "Iteration 1135, loss = 0.08087996\n",
      "Iteration 1136, loss = 0.08083810\n",
      "Iteration 1137, loss = 0.08069159\n",
      "Iteration 1138, loss = 0.08074073\n",
      "Iteration 1139, loss = 0.08052427\n",
      "Iteration 1140, loss = 0.08046824\n",
      "Iteration 1141, loss = 0.08041060\n",
      "Iteration 1142, loss = 0.08039320\n",
      "Iteration 1143, loss = 0.08013045\n",
      "Iteration 1144, loss = 0.08002361\n",
      "Iteration 1145, loss = 0.07994623\n",
      "Iteration 1146, loss = 0.07986547\n",
      "Iteration 1147, loss = 0.07974564\n",
      "Iteration 1148, loss = 0.07968943\n",
      "Iteration 1149, loss = 0.07968988\n",
      "Iteration 1150, loss = 0.07971394\n",
      "Iteration 1151, loss = 0.07946561\n",
      "Iteration 1152, loss = 0.07930222\n",
      "Iteration 1153, loss = 0.07955130\n",
      "Iteration 1154, loss = 0.07933562\n",
      "Iteration 1155, loss = 0.07934137\n",
      "Iteration 1156, loss = 0.07914895\n",
      "Iteration 1157, loss = 0.07911483\n",
      "Iteration 1158, loss = 0.07879074\n",
      "Iteration 1159, loss = 0.07870210\n",
      "Iteration 1160, loss = 0.07858087\n",
      "Iteration 1161, loss = 0.07872454\n",
      "Iteration 1162, loss = 0.07854202\n",
      "Iteration 1163, loss = 0.07834773\n",
      "Iteration 1164, loss = 0.07834229\n",
      "Iteration 1165, loss = 0.07819475\n",
      "Iteration 1166, loss = 0.07812355\n",
      "Iteration 1167, loss = 0.07803769\n",
      "Iteration 1168, loss = 0.07801858\n",
      "Iteration 1169, loss = 0.07793684\n",
      "Iteration 1170, loss = 0.07775880\n",
      "Iteration 1171, loss = 0.07767692\n",
      "Iteration 1172, loss = 0.07763618\n",
      "Iteration 1173, loss = 0.07761939\n",
      "Iteration 1174, loss = 0.07748389\n",
      "Iteration 1175, loss = 0.07783554\n",
      "Iteration 1176, loss = 0.07739366\n",
      "Iteration 1177, loss = 0.07729412\n",
      "Iteration 1178, loss = 0.07715313\n",
      "Iteration 1179, loss = 0.07706099\n",
      "Iteration 1180, loss = 0.07697983\n",
      "Iteration 1181, loss = 0.07717138\n",
      "Iteration 1182, loss = 0.07692541\n",
      "Iteration 1183, loss = 0.07679169\n",
      "Iteration 1184, loss = 0.07658814\n",
      "Iteration 1185, loss = 0.07672634\n",
      "Iteration 1186, loss = 0.07672046\n",
      "Iteration 1187, loss = 0.07636386\n",
      "Iteration 1188, loss = 0.07635302\n",
      "Iteration 1189, loss = 0.07617013\n",
      "Iteration 1190, loss = 0.07618812\n",
      "Iteration 1191, loss = 0.07613721\n",
      "Iteration 1192, loss = 0.07607719\n",
      "Iteration 1193, loss = 0.07596979\n",
      "Iteration 1194, loss = 0.07611185\n",
      "Iteration 1195, loss = 0.07582446\n",
      "Iteration 1196, loss = 0.07583904\n",
      "Iteration 1197, loss = 0.07566703\n",
      "Iteration 1198, loss = 0.07550602\n",
      "Iteration 1199, loss = 0.07564158\n",
      "Iteration 1200, loss = 0.07543147\n",
      "Iteration 1201, loss = 0.07523182\n",
      "Iteration 1202, loss = 0.07542349\n",
      "Iteration 1203, loss = 0.07578475\n",
      "Iteration 1204, loss = 0.07520159\n",
      "Iteration 1205, loss = 0.07497081\n",
      "Iteration 1206, loss = 0.07521821\n",
      "Iteration 1207, loss = 0.07496996\n",
      "Iteration 1208, loss = 0.07489204\n",
      "Iteration 1209, loss = 0.07502173\n",
      "Iteration 1210, loss = 0.07474988\n",
      "Iteration 1211, loss = 0.07452350\n",
      "Iteration 1212, loss = 0.07450882\n",
      "Iteration 1213, loss = 0.07446175\n",
      "Iteration 1214, loss = 0.07430595\n",
      "Iteration 1215, loss = 0.07429145\n",
      "Iteration 1216, loss = 0.07425586\n",
      "Iteration 1217, loss = 0.07425192\n",
      "Iteration 1218, loss = 0.07414622\n",
      "Iteration 1219, loss = 0.07415910\n",
      "Iteration 1220, loss = 0.07407649\n",
      "Iteration 1221, loss = 0.07394435\n",
      "Iteration 1222, loss = 0.07395751\n",
      "Iteration 1223, loss = 0.07372942\n",
      "Iteration 1224, loss = 0.07362307\n",
      "Iteration 1225, loss = 0.07398332\n",
      "Iteration 1226, loss = 0.07352957\n",
      "Iteration 1227, loss = 0.07335614\n",
      "Iteration 1228, loss = 0.07385886\n",
      "Iteration 1229, loss = 0.07337982\n",
      "Iteration 1230, loss = 0.07327214\n",
      "Iteration 1231, loss = 0.07371225\n",
      "Iteration 1232, loss = 0.07304299\n",
      "Iteration 1233, loss = 0.07298747\n",
      "Iteration 1234, loss = 0.07309064\n",
      "Iteration 1235, loss = 0.07293051\n",
      "Iteration 1236, loss = 0.07284169\n",
      "Iteration 1237, loss = 0.07302562\n",
      "Iteration 1238, loss = 0.07268676\n",
      "Iteration 1239, loss = 0.07263327\n",
      "Iteration 1240, loss = 0.07255618\n",
      "Iteration 1241, loss = 0.07244213\n",
      "Iteration 1242, loss = 0.07242321\n",
      "Iteration 1243, loss = 0.07261681\n",
      "Iteration 1244, loss = 0.07242443\n",
      "Iteration 1245, loss = 0.07209533\n",
      "Iteration 1246, loss = 0.07215313\n",
      "Iteration 1247, loss = 0.07240899\n",
      "Iteration 1248, loss = 0.07212424\n",
      "Iteration 1249, loss = 0.07198880\n",
      "Iteration 1250, loss = 0.07202802\n",
      "Iteration 1251, loss = 0.07180646\n",
      "Iteration 1252, loss = 0.07171863\n",
      "Iteration 1253, loss = 0.07182185\n",
      "Iteration 1254, loss = 0.07170771\n",
      "Iteration 1255, loss = 0.07166946\n",
      "Iteration 1256, loss = 0.07154377\n",
      "Iteration 1257, loss = 0.07139154\n",
      "Iteration 1258, loss = 0.07130953\n",
      "Iteration 1259, loss = 0.07172305\n",
      "Iteration 1260, loss = 0.07123603\n",
      "Iteration 1261, loss = 0.07121332\n",
      "Iteration 1262, loss = 0.07124929\n",
      "Iteration 1263, loss = 0.07104636\n",
      "Iteration 1264, loss = 0.07105113\n",
      "Iteration 1265, loss = 0.07126538\n",
      "Iteration 1266, loss = 0.07133084\n",
      "Iteration 1267, loss = 0.07079630\n",
      "Iteration 1268, loss = 0.07084766\n",
      "Iteration 1269, loss = 0.07096509\n",
      "Iteration 1270, loss = 0.07066462\n",
      "Iteration 1271, loss = 0.07077706\n",
      "Iteration 1272, loss = 0.07047767\n",
      "Iteration 1273, loss = 0.07047108\n",
      "Iteration 1274, loss = 0.07038278\n",
      "Iteration 1275, loss = 0.07053763\n",
      "Iteration 1276, loss = 0.07025639\n",
      "Iteration 1277, loss = 0.07037966\n",
      "Iteration 1278, loss = 0.07026952\n",
      "Iteration 1279, loss = 0.07031863\n",
      "Iteration 1280, loss = 0.07008784\n",
      "Iteration 1281, loss = 0.06990788\n",
      "Iteration 1282, loss = 0.07003806\n",
      "Iteration 1283, loss = 0.06990367\n",
      "Iteration 1284, loss = 0.06975848\n",
      "Iteration 1285, loss = 0.06973884\n",
      "Iteration 1286, loss = 0.06984971\n",
      "Iteration 1287, loss = 0.06972439\n",
      "Iteration 1288, loss = 0.06975143\n",
      "Iteration 1289, loss = 0.06971406\n",
      "Iteration 1290, loss = 0.06952203\n",
      "Iteration 1291, loss = 0.06959465\n",
      "Iteration 1292, loss = 0.06926297\n",
      "Iteration 1293, loss = 0.06927542\n",
      "Iteration 1294, loss = 0.06941692\n",
      "Iteration 1295, loss = 0.06959283\n",
      "Iteration 1296, loss = 0.06925043\n",
      "Iteration 1297, loss = 0.06923863\n",
      "Iteration 1298, loss = 0.06932949\n",
      "Iteration 1299, loss = 0.06905933\n",
      "Iteration 1300, loss = 0.06893741\n",
      "Iteration 1301, loss = 0.06881676\n",
      "Iteration 1302, loss = 0.06887163\n",
      "Iteration 1303, loss = 0.06885917\n",
      "Iteration 1304, loss = 0.06881000\n",
      "Iteration 1305, loss = 0.06870845\n",
      "Iteration 1306, loss = 0.06852657\n",
      "Iteration 1307, loss = 0.06871465\n",
      "Iteration 1308, loss = 0.06887185\n",
      "Iteration 1309, loss = 0.06861970\n",
      "Iteration 1310, loss = 0.06837303\n",
      "Iteration 1311, loss = 0.06867381\n",
      "Iteration 1312, loss = 0.06819889\n",
      "Iteration 1313, loss = 0.06813434\n",
      "Iteration 1314, loss = 0.06816769\n",
      "Iteration 1315, loss = 0.06821092\n",
      "Iteration 1316, loss = 0.06823027\n",
      "Iteration 1317, loss = 0.06792455\n",
      "Iteration 1318, loss = 0.06786773\n",
      "Iteration 1319, loss = 0.06791005\n",
      "Iteration 1320, loss = 0.06778567\n",
      "Iteration 1321, loss = 0.06784068\n",
      "Iteration 1322, loss = 0.06800065\n",
      "Iteration 1323, loss = 0.06780164\n",
      "Iteration 1324, loss = 0.06774860\n",
      "Iteration 1325, loss = 0.06769647\n",
      "Iteration 1326, loss = 0.06769820\n",
      "Iteration 1327, loss = 0.06743280\n",
      "Iteration 1328, loss = 0.06738600\n",
      "Iteration 1329, loss = 0.06739497\n",
      "Iteration 1330, loss = 0.06726408\n",
      "Iteration 1331, loss = 0.06738493\n",
      "Iteration 1332, loss = 0.06757493\n",
      "Iteration 1333, loss = 0.06712303\n",
      "Iteration 1334, loss = 0.06713190\n",
      "Iteration 1335, loss = 0.06710266\n",
      "Iteration 1336, loss = 0.06707042\n",
      "Iteration 1337, loss = 0.06691589\n",
      "Iteration 1338, loss = 0.06694952\n",
      "Iteration 1339, loss = 0.06686712\n",
      "Iteration 1340, loss = 0.06715353\n",
      "Iteration 1341, loss = 0.06697875\n",
      "Iteration 1342, loss = 0.06678427\n",
      "Iteration 1343, loss = 0.06662753\n",
      "Iteration 1344, loss = 0.06667287\n",
      "Iteration 1345, loss = 0.06650167\n",
      "Iteration 1346, loss = 0.06649426\n",
      "Iteration 1347, loss = 0.06640698\n",
      "Iteration 1348, loss = 0.06637579\n",
      "Iteration 1349, loss = 0.06634311\n",
      "Iteration 1350, loss = 0.06665010\n",
      "Iteration 1351, loss = 0.06615528\n",
      "Iteration 1352, loss = 0.06618449\n",
      "Iteration 1353, loss = 0.06620418\n",
      "Iteration 1354, loss = 0.06626417\n",
      "Iteration 1355, loss = 0.06605649\n",
      "Iteration 1356, loss = 0.06617840\n",
      "Iteration 1357, loss = 0.06637560\n",
      "Iteration 1358, loss = 0.06594530\n",
      "Iteration 1359, loss = 0.06582940\n",
      "Iteration 1360, loss = 0.06592740\n",
      "Iteration 1361, loss = 0.06572019\n",
      "Iteration 1362, loss = 0.06586214\n",
      "Iteration 1363, loss = 0.06577696\n",
      "Iteration 1364, loss = 0.06565669\n",
      "Iteration 1365, loss = 0.06557114\n",
      "Iteration 1366, loss = 0.06579623\n",
      "Iteration 1367, loss = 0.06570673\n",
      "Iteration 1368, loss = 0.06549403\n",
      "Iteration 1369, loss = 0.06549752\n",
      "Iteration 1370, loss = 0.06550806\n",
      "Iteration 1371, loss = 0.06551474\n",
      "Iteration 1372, loss = 0.06541745\n",
      "Iteration 1373, loss = 0.06536897\n",
      "Iteration 1374, loss = 0.06515897\n",
      "Iteration 1375, loss = 0.06516345\n",
      "Iteration 1376, loss = 0.06527851\n",
      "Iteration 1377, loss = 0.06503526\n",
      "Iteration 1378, loss = 0.06505381\n",
      "Iteration 1379, loss = 0.06489644\n",
      "Iteration 1380, loss = 0.06490387\n",
      "Iteration 1381, loss = 0.06485542\n",
      "Iteration 1382, loss = 0.06482924\n",
      "Iteration 1383, loss = 0.06480590\n",
      "Iteration 1384, loss = 0.06482487\n",
      "Iteration 1385, loss = 0.06471830\n",
      "Iteration 1386, loss = 0.06476605\n",
      "Iteration 1387, loss = 0.06478513\n",
      "Iteration 1388, loss = 0.06451367\n",
      "Iteration 1389, loss = 0.06451383\n",
      "Iteration 1390, loss = 0.06461484\n",
      "Iteration 1391, loss = 0.06475342\n",
      "Iteration 1392, loss = 0.06450308\n",
      "Iteration 1393, loss = 0.06433470\n",
      "Iteration 1394, loss = 0.06427381\n",
      "Iteration 1395, loss = 0.06420807\n",
      "Iteration 1396, loss = 0.06418965\n",
      "Iteration 1397, loss = 0.06475945\n",
      "Iteration 1398, loss = 0.06427254\n",
      "Iteration 1399, loss = 0.06421058\n",
      "Iteration 1400, loss = 0.06401834\n",
      "Iteration 1401, loss = 0.06420143\n",
      "Iteration 1402, loss = 0.06397145\n",
      "Iteration 1403, loss = 0.06404622\n",
      "Iteration 1404, loss = 0.06442713\n",
      "Iteration 1405, loss = 0.06384833\n",
      "Iteration 1406, loss = 0.06403558\n",
      "Iteration 1407, loss = 0.06382675\n",
      "Iteration 1408, loss = 0.06380888\n",
      "Iteration 1409, loss = 0.06366268\n",
      "Iteration 1410, loss = 0.06357321\n",
      "Iteration 1411, loss = 0.06357721\n",
      "Iteration 1412, loss = 0.06353658\n",
      "Iteration 1413, loss = 0.06386324\n",
      "Iteration 1414, loss = 0.06359944\n",
      "Iteration 1415, loss = 0.06342115\n",
      "Iteration 1416, loss = 0.06347155\n",
      "Iteration 1417, loss = 0.06331940\n",
      "Iteration 1418, loss = 0.06339864\n",
      "Iteration 1419, loss = 0.06321009\n",
      "Iteration 1420, loss = 0.06356747\n",
      "Iteration 1421, loss = 0.06325069\n",
      "Iteration 1422, loss = 0.06327026\n",
      "Iteration 1423, loss = 0.06355111\n",
      "Iteration 1424, loss = 0.06325558\n",
      "Iteration 1425, loss = 0.06316837\n",
      "Iteration 1426, loss = 0.06296975\n",
      "Iteration 1427, loss = 0.06289757\n",
      "Iteration 1428, loss = 0.06288401\n",
      "Iteration 1429, loss = 0.06283153\n",
      "Iteration 1430, loss = 0.06280589\n",
      "Iteration 1431, loss = 0.06273609\n",
      "Iteration 1432, loss = 0.06272892\n",
      "Iteration 1433, loss = 0.06273284\n",
      "Iteration 1434, loss = 0.06265975\n",
      "Iteration 1435, loss = 0.06259538\n",
      "Iteration 1436, loss = 0.06264927\n",
      "Iteration 1437, loss = 0.06246237\n",
      "Iteration 1438, loss = 0.06249993\n",
      "Iteration 1439, loss = 0.06269862\n",
      "Iteration 1440, loss = 0.06264140\n",
      "Iteration 1441, loss = 0.06250499\n",
      "Iteration 1442, loss = 0.06235670\n",
      "Iteration 1443, loss = 0.06253528\n",
      "Iteration 1444, loss = 0.06238707\n",
      "Iteration 1445, loss = 0.06258358\n",
      "Iteration 1446, loss = 0.06246725\n",
      "Iteration 1447, loss = 0.06212640\n",
      "Iteration 1448, loss = 0.06227009\n",
      "Iteration 1449, loss = 0.06210153\n",
      "Iteration 1450, loss = 0.06214203\n",
      "Iteration 1451, loss = 0.06221598\n",
      "Iteration 1452, loss = 0.06193739\n",
      "Iteration 1453, loss = 0.06196240\n",
      "Iteration 1454, loss = 0.06186914\n",
      "Iteration 1455, loss = 0.06202633\n",
      "Iteration 1456, loss = 0.06185197\n",
      "Iteration 1457, loss = 0.06178519\n",
      "Iteration 1458, loss = 0.06170837\n",
      "Iteration 1459, loss = 0.06179038\n",
      "Iteration 1460, loss = 0.06162146\n",
      "Iteration 1461, loss = 0.06174141\n",
      "Iteration 1462, loss = 0.06176319\n",
      "Iteration 1463, loss = 0.06177728\n",
      "Iteration 1464, loss = 0.06161291\n",
      "Iteration 1465, loss = 0.06149860\n",
      "Iteration 1466, loss = 0.06153092\n",
      "Iteration 1467, loss = 0.06151875\n",
      "Iteration 1468, loss = 0.06137909\n",
      "Iteration 1469, loss = 0.06139902\n",
      "Iteration 1470, loss = 0.06156928\n",
      "Iteration 1471, loss = 0.06153183\n",
      "Iteration 1472, loss = 0.06159635\n",
      "Iteration 1473, loss = 0.06133901\n",
      "Iteration 1474, loss = 0.06119370\n",
      "Iteration 1475, loss = 0.06132926\n",
      "Iteration 1476, loss = 0.06145088\n",
      "Iteration 1477, loss = 0.06110704\n",
      "Iteration 1478, loss = 0.06111777\n",
      "Iteration 1479, loss = 0.06103696\n",
      "Iteration 1480, loss = 0.06120920\n",
      "Iteration 1481, loss = 0.06099114\n",
      "Iteration 1482, loss = 0.06097972\n",
      "Iteration 1483, loss = 0.06091419\n",
      "Iteration 1484, loss = 0.06088841\n",
      "Iteration 1485, loss = 0.06085713\n",
      "Iteration 1486, loss = 0.06070761\n",
      "Iteration 1487, loss = 0.06072503\n",
      "Iteration 1488, loss = 0.06066973\n",
      "Iteration 1489, loss = 0.06070423\n",
      "Iteration 1490, loss = 0.06072440\n",
      "Iteration 1491, loss = 0.06065786\n",
      "Iteration 1492, loss = 0.06088453\n",
      "Iteration 1493, loss = 0.06058187\n",
      "Iteration 1494, loss = 0.06048171\n",
      "Iteration 1495, loss = 0.06077989\n",
      "Iteration 1496, loss = 0.06053024\n",
      "Iteration 1497, loss = 0.06050528\n",
      "Iteration 1498, loss = 0.06053004\n",
      "Iteration 1499, loss = 0.06042148\n",
      "Iteration 1500, loss = 0.06084641\n",
      "Iteration 1501, loss = 0.06056582\n",
      "Iteration 1502, loss = 0.06026796\n",
      "Iteration 1503, loss = 0.06056952\n",
      "Iteration 1504, loss = 0.06016964\n",
      "Iteration 1505, loss = 0.06026700\n",
      "Iteration 1506, loss = 0.06016837\n",
      "Iteration 1507, loss = 0.06004004\n",
      "Iteration 1508, loss = 0.06023394\n",
      "Iteration 1509, loss = 0.05999042\n",
      "Iteration 1510, loss = 0.06000547\n",
      "Iteration 1511, loss = 0.06000241\n",
      "Iteration 1512, loss = 0.05998893\n",
      "Iteration 1513, loss = 0.05989565\n",
      "Iteration 1514, loss = 0.05992255\n",
      "Iteration 1515, loss = 0.05978478\n",
      "Iteration 1516, loss = 0.05971676\n",
      "Iteration 1517, loss = 0.05973175\n",
      "Iteration 1518, loss = 0.05983286\n",
      "Iteration 1519, loss = 0.05986470\n",
      "Iteration 1520, loss = 0.05991675\n",
      "Iteration 1521, loss = 0.05977657\n",
      "Iteration 1522, loss = 0.05964994\n",
      "Iteration 1523, loss = 0.05998732\n",
      "Iteration 1524, loss = 0.05958564\n",
      "Iteration 1525, loss = 0.05947773\n",
      "Iteration 1526, loss = 0.05953497\n",
      "Iteration 1527, loss = 0.05976978\n",
      "Iteration 1528, loss = 0.06005402\n",
      "Iteration 1529, loss = 0.05958597\n",
      "Iteration 1530, loss = 0.05944697\n",
      "Iteration 1531, loss = 0.05956430\n",
      "Iteration 1532, loss = 0.05926766\n",
      "Iteration 1533, loss = 0.05925657\n",
      "Iteration 1534, loss = 0.05924589\n",
      "Iteration 1535, loss = 0.05934463\n",
      "Iteration 1536, loss = 0.05919937\n",
      "Iteration 1537, loss = 0.05918071\n",
      "Iteration 1538, loss = 0.05916610\n",
      "Iteration 1539, loss = 0.05913604\n",
      "Iteration 1540, loss = 0.05914915\n",
      "Iteration 1541, loss = 0.05905075\n",
      "Iteration 1542, loss = 0.05924947\n",
      "Iteration 1543, loss = 0.05914227\n",
      "Iteration 1544, loss = 0.05894791\n",
      "Iteration 1545, loss = 0.05886826\n",
      "Iteration 1546, loss = 0.05885032\n",
      "Iteration 1547, loss = 0.05893473\n",
      "Iteration 1548, loss = 0.05914120\n",
      "Iteration 1549, loss = 0.05878648\n",
      "Iteration 1550, loss = 0.05902466\n",
      "Iteration 1551, loss = 0.05879713\n",
      "Iteration 1552, loss = 0.05882218\n",
      "Iteration 1553, loss = 0.05868580\n",
      "Iteration 1554, loss = 0.05877995\n",
      "Iteration 1555, loss = 0.05909708\n",
      "Iteration 1556, loss = 0.05871461\n",
      "Iteration 1557, loss = 0.05858474\n",
      "Iteration 1558, loss = 0.05864587\n",
      "Iteration 1559, loss = 0.05850604\n",
      "Iteration 1560, loss = 0.05852660\n",
      "Iteration 1561, loss = 0.05845590\n",
      "Iteration 1562, loss = 0.05840324\n",
      "Iteration 1563, loss = 0.05840962\n",
      "Iteration 1564, loss = 0.05843182\n",
      "Iteration 1565, loss = 0.05856014\n",
      "Iteration 1566, loss = 0.05832933\n",
      "Iteration 1567, loss = 0.05813550\n",
      "Iteration 1568, loss = 0.05822378\n",
      "Iteration 1569, loss = 0.05825017\n",
      "Iteration 1570, loss = 0.05833185\n",
      "Iteration 1571, loss = 0.05819529\n",
      "Iteration 1572, loss = 0.05841159\n",
      "Iteration 1573, loss = 0.05823691\n",
      "Iteration 1574, loss = 0.05802079\n",
      "Iteration 1575, loss = 0.05801956\n",
      "Iteration 1576, loss = 0.05878711\n",
      "Iteration 1577, loss = 0.05806892\n",
      "Iteration 1578, loss = 0.05840985\n",
      "Iteration 1579, loss = 0.05817480\n",
      "Iteration 1580, loss = 0.05794074\n",
      "Iteration 1581, loss = 0.05805403\n",
      "Iteration 1582, loss = 0.05815745\n",
      "Iteration 1583, loss = 0.05796573\n",
      "Iteration 1584, loss = 0.05781167\n",
      "Iteration 1585, loss = 0.05778975\n",
      "Iteration 1586, loss = 0.05789988\n",
      "Iteration 1587, loss = 0.05805868\n",
      "Iteration 1588, loss = 0.05777082\n",
      "Iteration 1589, loss = 0.05776692\n",
      "Iteration 1590, loss = 0.05766511\n",
      "Iteration 1591, loss = 0.05827019\n",
      "Iteration 1592, loss = 0.05803943\n",
      "Iteration 1593, loss = 0.05760359\n",
      "Iteration 1594, loss = 0.05758986\n",
      "Iteration 1595, loss = 0.05759799\n",
      "Iteration 1596, loss = 0.05751048\n",
      "Iteration 1597, loss = 0.05784160\n",
      "Iteration 1598, loss = 0.05790302\n",
      "Iteration 1599, loss = 0.05756258\n",
      "Iteration 1600, loss = 0.05741927\n",
      "Iteration 1601, loss = 0.05736294\n",
      "Iteration 1602, loss = 0.05738848\n",
      "Iteration 1603, loss = 0.05733149\n",
      "Iteration 1604, loss = 0.05725171\n",
      "Iteration 1605, loss = 0.05724945\n",
      "Iteration 1606, loss = 0.05736396\n",
      "Iteration 1607, loss = 0.05715186\n",
      "Iteration 1608, loss = 0.05715799\n",
      "Iteration 1609, loss = 0.05740148\n",
      "Iteration 1610, loss = 0.05714523\n",
      "Iteration 1611, loss = 0.05714457\n",
      "Iteration 1612, loss = 0.05718200\n",
      "Iteration 1613, loss = 0.05704879\n",
      "Iteration 1614, loss = 0.05740432\n",
      "Iteration 1615, loss = 0.05715290\n",
      "Iteration 1616, loss = 0.05701285\n",
      "Iteration 1617, loss = 0.05746823\n",
      "Iteration 1618, loss = 0.05767021\n",
      "Iteration 1619, loss = 0.05704499\n",
      "Iteration 1620, loss = 0.05692947\n",
      "Iteration 1621, loss = 0.05689098\n",
      "Iteration 1622, loss = 0.05673374\n",
      "Iteration 1623, loss = 0.05693214\n",
      "Iteration 1624, loss = 0.05731576\n",
      "Iteration 1625, loss = 0.05684659\n",
      "Iteration 1626, loss = 0.05697857\n",
      "Iteration 1627, loss = 0.05670804\n",
      "Iteration 1628, loss = 0.05668151\n",
      "Iteration 1629, loss = 0.05676894\n",
      "Iteration 1630, loss = 0.05660662\n",
      "Iteration 1631, loss = 0.05664886\n",
      "Iteration 1632, loss = 0.05659754\n",
      "Iteration 1633, loss = 0.05658660\n",
      "Iteration 1634, loss = 0.05659137\n",
      "Iteration 1635, loss = 0.05680356\n",
      "Iteration 1636, loss = 0.05664799\n",
      "Iteration 1637, loss = 0.05665334\n",
      "Iteration 1638, loss = 0.05728456\n",
      "Iteration 1639, loss = 0.05639155\n",
      "Iteration 1640, loss = 0.05690146\n",
      "Iteration 1641, loss = 0.05620901\n",
      "Iteration 1642, loss = 0.05669971\n",
      "Iteration 1643, loss = 0.05687347\n",
      "Iteration 1644, loss = 0.05644786\n",
      "Iteration 1645, loss = 0.05676703\n",
      "Iteration 1646, loss = 0.05633372\n",
      "Iteration 1647, loss = 0.05635615\n",
      "Iteration 1648, loss = 0.05622996\n",
      "Iteration 1649, loss = 0.05630899\n",
      "Iteration 1650, loss = 0.05621152\n",
      "Iteration 1651, loss = 0.05609360\n",
      "Iteration 1652, loss = 0.05623837\n",
      "Iteration 1653, loss = 0.05608576\n",
      "Iteration 1654, loss = 0.05623742\n",
      "Iteration 1655, loss = 0.05602780\n",
      "Iteration 1656, loss = 0.05597821\n",
      "Iteration 1657, loss = 0.05605289\n",
      "Iteration 1658, loss = 0.05597753\n",
      "Iteration 1659, loss = 0.05597716\n",
      "Iteration 1660, loss = 0.05600415\n",
      "Iteration 1661, loss = 0.05591010\n",
      "Iteration 1662, loss = 0.05595973\n",
      "Iteration 1663, loss = 0.05591645\n",
      "Iteration 1664, loss = 0.05624171\n",
      "Iteration 1665, loss = 0.05619321\n",
      "Iteration 1666, loss = 0.05597129\n",
      "Iteration 1667, loss = 0.05588632\n",
      "Iteration 1668, loss = 0.05572785\n",
      "Iteration 1669, loss = 0.05599236\n",
      "Iteration 1670, loss = 0.05582713\n",
      "Iteration 1671, loss = 0.05577173\n",
      "Iteration 1672, loss = 0.05560237\n",
      "Iteration 1673, loss = 0.05591326\n",
      "Iteration 1674, loss = 0.05566201\n",
      "Iteration 1675, loss = 0.05575372\n",
      "Iteration 1676, loss = 0.05606430\n",
      "Iteration 1677, loss = 0.05573837\n",
      "Iteration 1678, loss = 0.05572423\n",
      "Iteration 1679, loss = 0.05561169\n",
      "Iteration 1680, loss = 0.05588049\n",
      "Iteration 1681, loss = 0.05588186\n",
      "Iteration 1682, loss = 0.05565133\n",
      "Iteration 1683, loss = 0.05542181\n",
      "Iteration 1684, loss = 0.05555744\n",
      "Iteration 1685, loss = 0.05555916\n",
      "Iteration 1686, loss = 0.05553166\n",
      "Iteration 1687, loss = 0.05538937\n",
      "Iteration 1688, loss = 0.05536026\n",
      "Iteration 1689, loss = 0.05550689\n",
      "Iteration 1690, loss = 0.05549618\n",
      "Iteration 1691, loss = 0.05540747\n",
      "Iteration 1692, loss = 0.05536748\n",
      "Iteration 1693, loss = 0.05524876\n",
      "Iteration 1694, loss = 0.05572966\n",
      "Iteration 1695, loss = 0.05528186\n",
      "Iteration 1696, loss = 0.05539836\n",
      "Iteration 1697, loss = 0.05531995\n",
      "Iteration 1698, loss = 0.05511330\n",
      "Iteration 1699, loss = 0.05527482\n",
      "Iteration 1700, loss = 0.05527394\n",
      "Iteration 1701, loss = 0.05502373\n",
      "Iteration 1702, loss = 0.05506843\n",
      "Iteration 1703, loss = 0.05507481\n",
      "Iteration 1704, loss = 0.05504161\n",
      "Iteration 1705, loss = 0.05501339\n",
      "Iteration 1706, loss = 0.05501835\n",
      "Iteration 1707, loss = 0.05513605\n",
      "Iteration 1708, loss = 0.05494907\n",
      "Iteration 1709, loss = 0.05515389\n",
      "Iteration 1710, loss = 0.05490160\n",
      "Iteration 1711, loss = 0.05486059\n",
      "Iteration 1712, loss = 0.05486299\n",
      "Iteration 1713, loss = 0.05512783\n",
      "Iteration 1714, loss = 0.05474571\n",
      "Iteration 1715, loss = 0.05479110\n",
      "Iteration 1716, loss = 0.05535208\n",
      "Iteration 1717, loss = 0.05498092\n",
      "Iteration 1718, loss = 0.05468455\n",
      "Iteration 1719, loss = 0.05474160\n",
      "Iteration 1720, loss = 0.05476149\n",
      "Iteration 1721, loss = 0.05488322\n",
      "Iteration 1722, loss = 0.05469302\n",
      "Iteration 1723, loss = 0.05468078\n",
      "Iteration 1724, loss = 0.05457340\n",
      "Iteration 1725, loss = 0.05461417\n",
      "Iteration 1726, loss = 0.05455987\n",
      "Iteration 1727, loss = 0.05475642\n",
      "Iteration 1728, loss = 0.05591954\n",
      "Iteration 1729, loss = 0.05451124\n",
      "Iteration 1730, loss = 0.05490814\n",
      "Iteration 1731, loss = 0.05498394\n",
      "Iteration 1732, loss = 0.05462112\n",
      "Iteration 1733, loss = 0.05437719\n",
      "Iteration 1734, loss = 0.05447733\n",
      "Iteration 1735, loss = 0.05491120\n",
      "Iteration 1736, loss = 0.05433828\n",
      "Iteration 1737, loss = 0.05444336\n",
      "Iteration 1738, loss = 0.05431226\n",
      "Iteration 1739, loss = 0.05446295\n",
      "Iteration 1740, loss = 0.05427131\n",
      "Iteration 1741, loss = 0.05433159\n",
      "Iteration 1742, loss = 0.05423090\n",
      "Iteration 1743, loss = 0.05421661\n",
      "Iteration 1744, loss = 0.05436087\n",
      "Iteration 1745, loss = 0.05445702\n",
      "Iteration 1746, loss = 0.05418352\n",
      "Iteration 1747, loss = 0.05419490\n",
      "Iteration 1748, loss = 0.05414916\n",
      "Iteration 1749, loss = 0.05432905\n",
      "Iteration 1750, loss = 0.05481924\n",
      "Iteration 1751, loss = 0.05407589\n",
      "Iteration 1752, loss = 0.05471048\n",
      "Iteration 1753, loss = 0.05415021\n",
      "Iteration 1754, loss = 0.05413200\n",
      "Iteration 1755, loss = 0.05427228\n",
      "Iteration 1756, loss = 0.05404792\n",
      "Iteration 1757, loss = 0.05392784\n",
      "Iteration 1758, loss = 0.05394561\n",
      "Iteration 1759, loss = 0.05391464\n",
      "Iteration 1760, loss = 0.05418806\n",
      "Iteration 1761, loss = 0.05413791\n",
      "Iteration 1762, loss = 0.05393681\n",
      "Iteration 1763, loss = 0.05379928\n",
      "Iteration 1764, loss = 0.05402469\n",
      "Iteration 1765, loss = 0.05405165\n",
      "Iteration 1766, loss = 0.05420849\n",
      "Iteration 1767, loss = 0.05385966\n",
      "Iteration 1768, loss = 0.05370899\n",
      "Iteration 1769, loss = 0.05371862\n",
      "Iteration 1770, loss = 0.05400876\n",
      "Iteration 1771, loss = 0.05404652\n",
      "Iteration 1772, loss = 0.05365071\n",
      "Iteration 1773, loss = 0.05383224\n",
      "Iteration 1774, loss = 0.05370127\n",
      "Iteration 1775, loss = 0.05406951\n",
      "Iteration 1776, loss = 0.05419403\n",
      "Iteration 1777, loss = 0.05366315\n",
      "Iteration 1778, loss = 0.05397192\n",
      "Iteration 1779, loss = 0.05352511\n",
      "Iteration 1780, loss = 0.05364425\n",
      "Iteration 1781, loss = 0.05362916\n",
      "Iteration 1782, loss = 0.05370397\n",
      "Iteration 1783, loss = 0.05382900\n",
      "Iteration 1784, loss = 0.05349930\n",
      "Iteration 1785, loss = 0.05388222\n",
      "Iteration 1786, loss = 0.05370818\n",
      "Iteration 1787, loss = 0.05347065\n",
      "Iteration 1788, loss = 0.05340818\n",
      "Iteration 1789, loss = 0.05336796\n",
      "Iteration 1790, loss = 0.05334998\n",
      "Iteration 1791, loss = 0.05352844\n",
      "Iteration 1792, loss = 0.05336371\n",
      "Iteration 1793, loss = 0.05330374\n",
      "Iteration 1794, loss = 0.05329713\n",
      "Iteration 1795, loss = 0.05350237\n",
      "Iteration 1796, loss = 0.05349150\n",
      "Iteration 1797, loss = 0.05341252\n",
      "Iteration 1798, loss = 0.05339807\n",
      "Iteration 1799, loss = 0.05347801\n",
      "Iteration 1800, loss = 0.05340796\n",
      "Iteration 1801, loss = 0.05336488\n",
      "Iteration 1802, loss = 0.05314343\n",
      "Iteration 1803, loss = 0.05311321\n",
      "Iteration 1804, loss = 0.05318863\n",
      "Iteration 1805, loss = 0.05308439\n",
      "Iteration 1806, loss = 0.05317672\n",
      "Iteration 1807, loss = 0.05311440\n",
      "Iteration 1808, loss = 0.05310903\n",
      "Iteration 1809, loss = 0.05340812\n",
      "Iteration 1810, loss = 0.05325517\n",
      "Iteration 1811, loss = 0.05305441\n",
      "Iteration 1812, loss = 0.05297295\n",
      "Iteration 1813, loss = 0.05293428\n",
      "Iteration 1814, loss = 0.05295886\n",
      "Iteration 1815, loss = 0.05300704\n",
      "Iteration 1816, loss = 0.05314685\n",
      "Iteration 1817, loss = 0.05328102\n",
      "Iteration 1818, loss = 0.05295166\n",
      "Iteration 1819, loss = 0.05292922\n",
      "Iteration 1820, loss = 0.05286788\n",
      "Iteration 1821, loss = 0.05285295\n",
      "Iteration 1822, loss = 0.05370339\n",
      "Iteration 1823, loss = 0.05374370\n",
      "Iteration 1824, loss = 0.05276050\n",
      "Iteration 1825, loss = 0.05294775\n",
      "Iteration 1826, loss = 0.05288377\n",
      "Iteration 1827, loss = 0.05284319\n",
      "Iteration 1828, loss = 0.05286869\n",
      "Iteration 1829, loss = 0.05285119\n",
      "Iteration 1830, loss = 0.05280017\n",
      "Iteration 1831, loss = 0.05275138\n",
      "Iteration 1832, loss = 0.05280064\n",
      "Iteration 1833, loss = 0.05266686\n",
      "Iteration 1834, loss = 0.05268099\n",
      "Iteration 1835, loss = 0.05261588\n",
      "Iteration 1836, loss = 0.05278899\n",
      "Iteration 1837, loss = 0.05273403\n",
      "Iteration 1838, loss = 0.05254383\n",
      "Iteration 1839, loss = 0.05259717\n",
      "Iteration 1840, loss = 0.05251958\n",
      "Iteration 1841, loss = 0.05270648\n",
      "Iteration 1842, loss = 0.05292093\n",
      "Iteration 1843, loss = 0.05258060\n",
      "Iteration 1844, loss = 0.05259261\n",
      "Iteration 1845, loss = 0.05248675\n",
      "Iteration 1846, loss = 0.05250182\n",
      "Iteration 1847, loss = 0.05245346\n",
      "Iteration 1848, loss = 0.05248068\n",
      "Iteration 1849, loss = 0.05244827\n",
      "Iteration 1850, loss = 0.05238323\n",
      "Iteration 1851, loss = 0.05235953\n",
      "Iteration 1852, loss = 0.05244394\n",
      "Iteration 1853, loss = 0.05235645\n",
      "Iteration 1854, loss = 0.05264170\n",
      "Iteration 1855, loss = 0.05234205\n",
      "Iteration 1856, loss = 0.05234274\n",
      "Iteration 1857, loss = 0.05218844\n",
      "Iteration 1858, loss = 0.05222724\n",
      "Iteration 1859, loss = 0.05235594\n",
      "Iteration 1860, loss = 0.05236523\n",
      "Iteration 1861, loss = 0.05259373\n",
      "Iteration 1862, loss = 0.05239340\n",
      "Iteration 1863, loss = 0.05245483\n",
      "Iteration 1864, loss = 0.05219763\n",
      "Iteration 1865, loss = 0.05269578\n",
      "Iteration 1866, loss = 0.05224043\n",
      "Iteration 1867, loss = 0.05278250\n",
      "Iteration 1868, loss = 0.05219531\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size=32, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(4, 5), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=2000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=1e-06, validation_fraction=0.1, verbose=True,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = MLPClassifier(max_iter=2000, \n",
    "                        verbose=True,\n",
    "                        tol=0.0000100,\n",
    "                        activation = 'logistic',\n",
    "                        solver = 'adam',\n",
    "                        learning_rate = 'constant',\n",
    "                        learning_rate_init = 0.001,\n",
    "                        batch_size = 32,\n",
    "                        hidden_layer_sizes = (4, 5)\n",
    "                        #early_stopping = True,\n",
    "                        #n_iter_no_change = 50\n",
    "                        )\n",
    "network.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XcEtz4qttV4M",
    "outputId": "a1c18ee8-7644-499f-deca-c85ff5c11e4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.29100000000016e-05"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.06410639 - 0.06401348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_ctazvolzZzi",
    "outputId": "1d936730-798b-4478-d300-0f002ebbc688"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "UdaztvVezhey",
    "outputId": "eeff5d93-feb7-4bf1-ce0c-a2867b97bdef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.5928957 , -0.77243448, -0.3928078 ,  0.74173876],\n",
       "        [ 0.85086592, -1.1809518 , -1.46760687,  1.18335109],\n",
       "        [-1.08969801,  1.17630548,  2.10742254, -1.22671479],\n",
       "        [-1.85215328,  2.55106225,  2.87460738, -2.07004364]]),\n",
       " array([[ 3.63300154, -2.52831757, -2.87800652,  4.0074379 ,  2.52822432],\n",
       "        [-3.1669718 ,  1.89008775,  1.6884073 , -3.70045987, -2.59777303],\n",
       "        [-0.75958335,  3.42817752,  3.78108173, -1.04661829, -4.36677743],\n",
       "        [ 3.39600808, -1.10189869, -1.35263448,  3.29967519,  1.61762488]]),\n",
       " array([[ 1.73908125,  1.73436315, -3.60548497],\n",
       "        [-3.39330493, -0.2604855 ,  1.87461225],\n",
       "        [-3.60511005,  0.08694188,  1.50770268],\n",
       "        [ 1.24722806,  1.93064801, -4.09529503],\n",
       "        [ 3.52320406, -3.22580853, -1.51377268]])]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "3zBpcda3zsqr",
    "outputId": "48f09b0a-9789-47f9-959b-91b0b2d19d41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.88841256, -1.4171068 , -0.7773102 ,  0.83637301]),\n",
       " array([ 0.05511642,  0.36856006,  1.20268737,  0.61106625, -1.1097951 ]),\n",
       " array([-0.37069609, -0.70991621,  0.93136602])]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "cjBTc9xiz03F",
    "outputId": "e890763a-6771-4a19-ce3c-9eb79628c536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "uoTMTgwoz4z6",
    "outputId": "3d1b1f18-4ff7-46a0-e0da-49ae944889e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.n_outputs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QE4uaxjuz8ln",
    "outputId": "25c772ee-285d-4cbd-9e49-41bd7c036fbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.out_activation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTaTfbXNmUrn"
   },
   "source": [
    "## Neural network (evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "d2YjQaZQ0GC8",
    "outputId": "8ba1ee5d-87d1-4932-8286-f9d87504a7da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "9w0ilRj00Le-",
    "outputId": "cea94ef4-a27c-4c09-9756-d7988d5f0d4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 2, 1,\n",
       "       1, 0, 2, 2, 2, 0, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = network.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "mlVIH7Rv0TXv",
    "outputId": "a0ccc5e6-62ec-4ffd-d4d4-b9a7e0517852"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 2, 0, 2, 1,\n",
       "       1, 0, 2, 2, 2, 0, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gnJdHw7K0aIv",
    "outputId": "a4b188ad-bb6a-47ad-ee0a-bc0ab44afc2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "Qm2U1TRd0kgw",
    "outputId": "3ed076f0-8d1f-4062-a51b-e6ced879bd6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  0,  0],\n",
       "       [ 0,  6,  0],\n",
       "       [ 0,  1, 12]])"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "SENV1W_q1EPJ",
    "outputId": "711acc9d-7038-431a-e843-16dd3268e026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yellowbrick\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/95/a14e4fdfb8b1c8753bbe74a626e910a98219ef9c87c6763585bbd30d84cf/yellowbrick-1.1-py3-none-any.whl (263kB)\n",
      "\r",
      "\u001b[K     |                              | 10kB 18.3MB/s eta 0:00:01\r",
      "\u001b[K     |                             | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |                            | 30kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |                           | 40kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |                         | 51kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K     |                        | 61kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |                       | 71kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |                      | 81kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |                    | 92kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |                   | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |                  | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |                 | 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |               | 133kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |              | 143kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |             | 153kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |            | 163kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |          | 174kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |         | 184kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |        | 194kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |       | 204kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |      | 215kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |    | 225kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |   | 235kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |  | 245kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     | | 256kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     || 266kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10.0->yellowbrick) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->yellowbrick) (0.15.1)\n",
      "Installing collected packages: yellowbrick\n",
      "  Found existing installation: yellowbrick 0.9.1\n",
      "    Uninstalling yellowbrick-0.9.1:\n",
      "      Successfully uninstalled yellowbrick-0.9.1\n",
      "Successfully installed yellowbrick-1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install yellowbrick --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "7nBXuAJb1A33",
    "outputId": "696be522-0c43-46c3-95b8-eab454d6b634"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa8febc4d30>"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "confusion_matrix = ConfusionMatrix(network, classes = iris.target_names)\n",
    "confusion_matrix.fit(X_train, y_train)\n",
    "confusion_matrix.score(X_test, y_test)\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nkf_QRkpmZ7u"
   },
   "source": [
    "## Neural network (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QqUpFkIf1XFO",
    "outputId": "a2dad010-8a01-4024-9f45-db685538778d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5. , 3.4, 1.5, 0.2]), 0)"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tr24nFnx1hzK",
    "outputId": "9da985f6-9925-4201-d9fa-eddcd8a94566"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pOJL7ixY1lHn",
    "outputId": "3a53b60f-84f8-494a-b768-17174b2f141e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = X_test[0].reshape(1, -1)\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pN6DUvXN1cxW",
    "outputId": "db7e29cd-5bfd-4990-d230-e85865952e08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.predict(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "8W4Q0zsk9yzR",
    "outputId": "13887cf9-8b06-4e21-cd12-d92bc40cd171"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names[network.predict(new)[0]]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Libraries for Neural Networks - sklearn 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
