{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# RNN for Text Generation\n",
    "\n",
    "## Generating Text (encoded variables)\n",
    "\n",
    "We saw how to generate continuous values, now let's see how to generalize this to generate categorical sequences (such as words or letters).\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TomSawyer.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE ADVENTURES OF TOM SAWYER\\n\\nBy Mark Twain\\n(Samuel Langhorne Clemens)\\n\\nPREFACE\\n\\nMost of the adventures recorded in this book really occurred; one or two\\nwere experiences of my own, the rest those of boys who were schoolmates\\nof mine. Huck Finn is drawn from life; Tom Sawyer also, but not from an\\nindividual--he is a combination of the characteristics of three boys whom\\nI knew, and therefore belongs to the composite order of architecture.\\n\\nThe odd superstitions touched upon were all prevalent among children and\\nslaves in the West at the period of this story--that is to say, thirty or\\nforty years ago.\\n\\nAlthough my book is intended mainly for the entertainment of boys and\\ngirls, I hope it will not be shunned by men and women on that account,\\nfor part of my plan has been to try to pleasantly remind adults of what\\nthey once were themselves, and of how they felt and thought and talked,\\nand what queer enterprises they sometimes engaged in.\\n\\nTHE AUTHOR.\\n\\nHARTFORD, 1876.\\n\\n\\nCHAPTER I\\n\\n“TOM!”\\n\\nNo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ADVENTURES OF TOM SAWYER\n",
      "\n",
      "By Mark Twain\n",
      "(Samuel Langhorne Clemens)\n",
      "\n",
      "PREFACE\n",
      "\n",
      "Most of the adventures recorded in this book really occurred; one or two\n",
      "were experiences of my own, the rest those of boys who were schoolmates\n",
      "of mine. Huck Finn is drawn from life; Tom Sawyer also, but not from an\n",
      "individual--he is a combination of the characteristics of three boys whom\n",
      "I knew, and therefore belongs to the composite order of architecture.\n",
      "\n",
      "The odd superstitions touched upon were all prevalent among children and\n",
      "slaves in the West at the period of this story--that is to say, thirty or\n",
      "forty years ago.\n",
      "\n",
      "Although my book is intended mainly for the entertainment of boys and\n",
      "girls, I hope it will not be shunned by men and women on that account,\n",
      "for part of my plan has been to try to pleasantly remind adults of what\n",
      "they once were themselves, and of how they felt and thought and talked,\n",
      "and what queer enterprises they sometimes engaged in.\n",
      "\n",
      "THE AUTHOR.\n",
      "\n",
      "HARTFORD, 1876.\n",
      "\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "“TOM!”\n",
      "\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406270"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Entire Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,1,1,2,2,2,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '“',\n",
       " '”'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'S',\n",
       " 1: '.',\n",
       " 2: '*',\n",
       " 3: 'z',\n",
       " 4: 'R',\n",
       " 5: 'I',\n",
       " 6: 'a',\n",
       " 7: '6',\n",
       " 8: 'l',\n",
       " 9: '?',\n",
       " 10: 'r',\n",
       " 11: '(',\n",
       " 12: ')',\n",
       " 13: 'Y',\n",
       " 14: 'O',\n",
       " 15: 'i',\n",
       " 16: 'k',\n",
       " 17: '”',\n",
       " 18: 'J',\n",
       " 19: 'y',\n",
       " 20: 'P',\n",
       " 21: 'u',\n",
       " 22: '5',\n",
       " 23: ':',\n",
       " 24: 'V',\n",
       " 25: 'T',\n",
       " 26: '4',\n",
       " 27: 'H',\n",
       " 28: 'F',\n",
       " 29: '8',\n",
       " 30: 'G',\n",
       " 31: 'X',\n",
       " 32: '\\n',\n",
       " 33: '%',\n",
       " 34: '[',\n",
       " 35: 'U',\n",
       " 36: 'w',\n",
       " 37: 'o',\n",
       " 38: 'A',\n",
       " 39: 'f',\n",
       " 40: '@',\n",
       " 41: '9',\n",
       " 42: 'x',\n",
       " 43: '/',\n",
       " 44: 'W',\n",
       " 45: 'g',\n",
       " 46: 'e',\n",
       " 47: 'c',\n",
       " 48: 'n',\n",
       " 49: 'm',\n",
       " 50: 'q',\n",
       " 51: 't',\n",
       " 52: ']',\n",
       " 53: 'L',\n",
       " 54: 'N',\n",
       " 55: '“',\n",
       " 56: ';',\n",
       " 57: 'j',\n",
       " 58: '3',\n",
       " 59: 'v',\n",
       " 60: 'h',\n",
       " 61: '!',\n",
       " 62: '7',\n",
       " 63: '&',\n",
       " 64: 'E',\n",
       " 65: '_',\n",
       " 66: 'D',\n",
       " 67: 'b',\n",
       " 68: 'Q',\n",
       " 69: '1',\n",
       " 70: '$',\n",
       " 71: '0',\n",
       " 72: 'C',\n",
       " 73: \"'\",\n",
       " 74: '-',\n",
       " 75: 'p',\n",
       " 76: 'd',\n",
       " 77: 'B',\n",
       " 78: 's',\n",
       " 79: ' ',\n",
       " 80: 'M',\n",
       " 81: 'K',\n",
       " 82: ',',\n",
       " 83: '2'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 'S'), (1, '.'), (2, '*'), (3, 'z'), (4, 'R'), (5, 'I'), (6, 'a'), (7, '6'), (8, 'l'), (9, '?'), (10, 'r'), (11, '('), (12, ')'), (13, 'Y'), (14, 'O'), (15, 'i'), (16, 'k'), (17, '”'), (18, 'J'), (19, 'y'), (20, 'P'), (21, 'u'), (22, '5'), (23, ':'), (24, 'V'), (25, 'T'), (26, '4'), (27, 'H'), (28, 'F'), (29, '8'), (30, 'G'), (31, 'X'), (32, '\\n'), (33, '%'), (34, '['), (35, 'U'), (36, 'w'), (37, 'o'), (38, 'A'), (39, 'f'), (40, '@'), (41, '9'), (42, 'x'), (43, '/'), (44, 'W'), (45, 'g'), (46, 'e'), (47, 'c'), (48, 'n'), (49, 'm'), (50, 'q'), (51, 't'), (52, ']'), (53, 'L'), (54, 'N'), (55, '“'), (56, ';'), (57, 'j'), (58, '3'), (59, 'v'), (60, 'h'), (61, '!'), (62, '7'), (63, '&'), (64, 'E'), (65, '_'), (66, 'D'), (67, 'b'), (68, 'Q'), (69, '1'), (70, '$'), (71, '0'), (72, 'C'), (73, \"'\"), (74, '-'), (75, 'p'), (76, 'd'), (77, 'B'), (78, 's'), (79, ' '), (80, 'M'), (81, 'K'), (82, ','), (83, '2')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 0,\n",
       " '.': 1,\n",
       " '*': 2,\n",
       " 'z': 3,\n",
       " 'R': 4,\n",
       " 'I': 5,\n",
       " 'a': 6,\n",
       " '6': 7,\n",
       " 'l': 8,\n",
       " '?': 9,\n",
       " 'r': 10,\n",
       " '(': 11,\n",
       " ')': 12,\n",
       " 'Y': 13,\n",
       " 'O': 14,\n",
       " 'i': 15,\n",
       " 'k': 16,\n",
       " '”': 17,\n",
       " 'J': 18,\n",
       " 'y': 19,\n",
       " 'P': 20,\n",
       " 'u': 21,\n",
       " '5': 22,\n",
       " ':': 23,\n",
       " 'V': 24,\n",
       " 'T': 25,\n",
       " '4': 26,\n",
       " 'H': 27,\n",
       " 'F': 28,\n",
       " '8': 29,\n",
       " 'G': 30,\n",
       " 'X': 31,\n",
       " '\\n': 32,\n",
       " '%': 33,\n",
       " '[': 34,\n",
       " 'U': 35,\n",
       " 'w': 36,\n",
       " 'o': 37,\n",
       " 'A': 38,\n",
       " 'f': 39,\n",
       " '@': 40,\n",
       " '9': 41,\n",
       " 'x': 42,\n",
       " '/': 43,\n",
       " 'W': 44,\n",
       " 'g': 45,\n",
       " 'e': 46,\n",
       " 'c': 47,\n",
       " 'n': 48,\n",
       " 'm': 49,\n",
       " 'q': 50,\n",
       " 't': 51,\n",
       " ']': 52,\n",
       " 'L': 53,\n",
       " 'N': 54,\n",
       " '“': 55,\n",
       " ';': 56,\n",
       " 'j': 57,\n",
       " '3': 58,\n",
       " 'v': 59,\n",
       " 'h': 60,\n",
       " '!': 61,\n",
       " '7': 62,\n",
       " '&': 63,\n",
       " 'E': 64,\n",
       " '_': 65,\n",
       " 'D': 66,\n",
       " 'b': 67,\n",
       " 'Q': 68,\n",
       " '1': 69,\n",
       " '$': 70,\n",
       " '0': 71,\n",
       " 'C': 72,\n",
       " \"'\": 73,\n",
       " '-': 74,\n",
       " 'p': 75,\n",
       " 'd': 76,\n",
       " 'B': 77,\n",
       " 's': 78,\n",
       " ' ': 79,\n",
       " 'M': 80,\n",
       " 'K': 81,\n",
       " ',': 82,\n",
       " '2': 83}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406270"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 27, 64, 79, 38, 66, 24, 64, 54, 25, 35,  4, 64,  0, 79, 14, 28,\n",
       "       79, 25, 14, 80, 79,  0, 38, 44, 13, 64,  4, 32, 32, 77, 19, 79, 80,\n",
       "        6, 10, 16, 79, 25, 36,  6, 15, 48, 32, 11,  0,  6, 49, 21, 46,  8,\n",
       "       79, 53,  6, 48, 45, 60, 37, 10, 48, 46, 79, 72,  8, 46, 49, 46, 48,\n",
       "       78, 12, 32, 32, 20,  4, 64, 28, 38, 72, 64, 32, 32, 80, 37, 78, 51,\n",
       "       79, 37, 39, 79, 51, 60, 46, 79,  6, 76, 59, 46, 48, 51, 21, 10, 46,\n",
       "       78, 79, 10, 46, 47, 37, 10, 76, 46, 76, 79, 15, 48, 79, 51, 60, 15,\n",
       "       78, 79, 67, 37, 37, 16, 79, 10, 46,  6,  8,  8, 19, 79, 37, 47, 47,\n",
       "       21, 10, 10, 46, 76, 56, 79, 37, 48, 46, 79, 37, 10, 79, 51, 36, 37,\n",
       "       32, 36, 46, 10, 46, 79, 46, 42, 75, 46, 10, 15, 46, 48, 47, 46, 78,\n",
       "       79, 37, 39, 79, 49, 19, 79, 37, 36, 48, 82, 79, 51, 60, 46, 79, 10,\n",
       "       46, 78, 51, 79, 51, 60, 37, 78, 46, 79, 37, 39, 79, 67, 37, 19, 78,\n",
       "       79, 36, 60, 37, 79, 36, 46, 10, 46, 79, 78, 47, 60, 37, 37,  8, 49,\n",
       "        6, 51, 46, 78, 32, 37, 39, 79, 49, 15, 48, 46,  1, 79, 27, 21, 47,\n",
       "       16, 79, 28, 15, 48, 48, 79, 15, 78, 79, 76, 10,  6, 36, 48, 79, 39,\n",
       "       10, 37, 49, 79,  8, 15, 39, 46, 56, 79, 25, 37, 49, 79,  0,  6, 36,\n",
       "       19, 46, 10, 79,  6,  8, 78, 37, 82, 79, 67, 21, 51, 79, 48, 37, 51,\n",
       "       79, 39, 10, 37, 49, 79,  6, 48, 32, 15, 48, 76, 15, 59, 15, 76, 21,\n",
       "        6,  8, 74, 74, 60, 46, 79, 15, 78, 79,  6, 79, 47, 37, 49, 67, 15,\n",
       "       48,  6, 51, 15, 37, 48, 79, 37, 39, 79, 51, 60, 46, 79, 47, 60,  6,\n",
       "       10,  6, 47, 51, 46, 10, 15, 78, 51, 15, 47, 78, 79, 37, 39, 79, 51,\n",
       "       60, 10, 46, 46, 79, 67, 37, 19, 78, 79, 36, 60, 37, 49, 32,  5, 79,\n",
       "       16, 48, 46, 36, 82, 79,  6, 48, 76, 79, 51, 60, 46, 10, 46, 39, 37,\n",
       "       10, 46, 79, 67, 46,  8, 37, 48, 45, 78, 79, 51, 37, 79, 51, 60, 46,\n",
       "       79, 47, 37, 49, 75, 37, 78, 15, 51, 46, 79, 37, 10, 76, 46, 10, 79,\n",
       "       37, 39, 79,  6, 10, 47, 60, 15, 51, 46, 47, 51, 21, 10, 46,  1, 32,\n",
       "       32, 25, 60, 46, 79, 37, 76, 76, 79, 78, 21, 75, 46, 10, 78, 51, 15,\n",
       "       51, 15, 37, 48, 78, 79, 51, 37, 21, 47, 60, 46, 76, 79, 21, 75, 37,\n",
       "       48, 79, 36, 46, 10, 46, 79,  6,  8,  8, 79, 75, 10, 46, 59,  6,  8,\n",
       "       46, 48, 51, 79,  6, 49, 37])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ADVENTURES OF TOM SAWYER\n",
      "\n",
      "By Mark Twain\n",
      "(Samuel Langhorne Clemens)\n",
      "\n",
      "PREFACE\n",
      "\n",
      "Most of the adventures recorded in this book really occurred; one or two\n",
      "were experiences of my own, the rest those of boys who were schoolmates\n",
      "of mine. Huck Finn is drawn from life; Tom Sawyer also, but not from an\n",
      "individual--he is a combination of the characteristics of three boys whom\n",
      "I knew, and therefore belongs to the composite order of architecture.\n",
      "\n",
      "The odd superstitions touched upon were all prevalent amo\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "As previously discussed, we need to one-hot encode our data inorder for it to work with the network structure. Make sure to review numpy if any of these operations confuse you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 1, 1,2,7,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.zeros((a.size, a.max()+1))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[np.arange(a.size),a] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    '''\n",
    "    encoded_text : batch of encoded text\n",
    "    \n",
    "    num_uni_chars = number of unique characters (len(set(text)))\n",
    "    '''\n",
    "    \n",
    "    # METHOD FROM:\n",
    "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
    "      \n",
    "    # Create a placeholder for zeros.\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # Convert data type for later use with pytorch (errors if we dont!)\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # Using fancy indexing fill in the 1s at the correct index locations\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "\n",
    "    # Reshape it so it matches the batch sahe\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(np.array([0,1,2]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(encoded_text.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_text.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(406269/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500*812"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "---------------\n",
    "# Creating Training Batches\n",
    "\n",
    "We need to create a function that will generate batches of characters along with the next character in the sequence as a label.\n",
    "\n",
    "-----------------\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we wanted 5 batches\n",
    "example_text.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_per_batch=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_per_batch = samp_per_batch * seq_len\n",
    "# char_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of batches available to make\n",
    "# Use int() to round to nearest integer\n",
    "# num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "# num_batches_avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Cut off end of encoded_text that\n",
    "#     won't fit evenly into a batch\n",
    "#     encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "#     len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Reshape text into rows the size of a batch\n",
    "#     encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "#     encoded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = range(0, encoded_text.shape[1], seq_len)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "    # characters come out per batch.\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # Use int() to round to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of generating a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 27, 64, 79, 38, 66, 24, 64, 54, 25, 35,  4, 64,  0, 79, 14, 28,\n",
       "       79, 25, 14])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(encoded_text,samp_per_batch=10,seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab first batch\n",
    "x, y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 27, 64, 79, 38, 66, 24, 64, 54, 25, 35,  4, 64,  0, 79, 14,\n",
       "        28, 79, 25, 14, 80, 79,  0, 38, 44, 13, 64,  4, 32, 32, 77, 19,\n",
       "        79, 80,  6, 10, 16, 79, 25, 36,  6, 15, 48, 32, 11,  0,  6, 49,\n",
       "        21, 46],\n",
       "       [48, 76, 79,  6, 79, 67, 10, 37, 51, 60, 46, 10, 82, 79, 36, 15,\n",
       "        51, 60, 37, 21, 51, 79, 76, 15, 78, 51, 15, 48, 47, 51, 15, 37,\n",
       "        48, 79, 37, 39, 32, 47, 37,  8, 37, 10, 82, 79,  6, 48, 76, 79,\n",
       "        60, 15],\n",
       "       [76, 46, 10, 79, 39, 37,  8,  8, 37, 36, 46, 76, 23, 32, 32, 55,\n",
       "        54, 37, 36, 82, 79, 78, 15, 10, 82, 79, 45, 37, 79,  6, 48, 76,\n",
       "        79, 78, 15, 51, 79, 36, 15, 51, 60, 79, 51, 60, 46, 79, 45, 15,\n",
       "        10,  8],\n",
       "       [15, 78, 79, 16, 46, 46, 75, 79, 21, 78, 79, 39, 10, 37, 49, 79,\n",
       "        65, 46, 59, 46, 10, 65, 32, 51, 46,  8,  8, 15, 48, 45, 74, 74,\n",
       "        65,  6,  8, 36,  6, 19, 78, 65,  9, 17, 32, 32, 55, 14, 39, 79,\n",
       "        47, 37],\n",
       "       [79,  6, 48, 76, 79, 46, 19, 46, 76, 79, 51, 60, 46, 32, 78, 51,\n",
       "        10,  6, 48, 45, 46, 10, 78, 79, 36, 15, 51, 60, 79,  6, 79, 47,\n",
       "        37, 48, 78, 21, 49, 15, 48, 45, 79, 47, 21, 10, 15, 37, 78, 15,\n",
       "        51, 19],\n",
       "       [76, 79, 51, 60, 46, 48, 79,  6, 48, 37, 51, 60, 46, 10, 79, 75,\n",
       "         6, 15, 10, 32, 37, 39, 79, 46, 19, 46, 78, 79, 39, 37,  8,  8,\n",
       "        37, 36, 46, 76, 79, 51, 60, 46, 79, 49, 15, 48, 15, 78, 51, 46,\n",
       "        10, 73],\n",
       "       [48, 51, 46, 49, 75,  8,  6, 51, 46, 79, 51, 60, 46, 79, 51, 36,\n",
       "        37, 79, 67, 46, 15, 48, 45, 78, 79, 75, 10, 46, 78, 46, 48, 51,\n",
       "        46, 76,  1, 17, 32, 32, 25, 60, 15, 78, 79, 48, 15, 45, 60, 51,\n",
       "        49,  6],\n",
       "       [21, 75, 37, 48, 79, 60, 15, 78, 79, 16, 48, 46, 46, 78, 74, 74,\n",
       "        78, 51, 15, 10, 10, 46, 76, 79, 60, 15, 49, 79, 21, 75, 79, 36,\n",
       "        15, 51, 60, 32, 60, 15, 78, 79, 39, 37, 37, 51, 79,  6, 48, 76,\n",
       "        79, 78],\n",
       "       [79, 76, 15, 10, 46, 47, 51, 15, 37, 48, 82, 79, 48, 37, 36, 56,\n",
       "        79, 51, 60, 46, 79, 51, 10, 46,  6, 78, 21, 10, 46, 79, 49, 21,\n",
       "        78, 51, 79, 67, 46, 79, 78, 51, 15,  8,  8, 32, 15, 48, 79, 54,\n",
       "        37,  1],\n",
       "       [79, 51, 60, 46, 79, 47, 10, 37, 78, 78, 65, 82, 73, 79, 60, 46,\n",
       "        19,  9, 79,  4, 15, 45, 60, 51, 79, 19, 37, 48, 76, 46, 10, 73,\n",
       "        78, 32, 36, 60, 46, 10, 46, 79,  5, 79, 78,  6, 36, 79,  5, 48,\n",
       "        57, 21]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 64, 79, 38, 66, 24, 64, 54, 25, 35,  4, 64,  0, 79, 14, 28,\n",
       "        79, 25, 14, 80, 79,  0, 38, 44, 13, 64,  4, 32, 32, 77, 19, 79,\n",
       "        80,  6, 10, 16, 79, 25, 36,  6, 15, 48, 32, 11,  0,  6, 49, 21,\n",
       "        46,  8],\n",
       "       [76, 79,  6, 79, 67, 10, 37, 51, 60, 46, 10, 82, 79, 36, 15, 51,\n",
       "        60, 37, 21, 51, 79, 76, 15, 78, 51, 15, 48, 47, 51, 15, 37, 48,\n",
       "        79, 37, 39, 32, 47, 37,  8, 37, 10, 82, 79,  6, 48, 76, 79, 60,\n",
       "        15, 78],\n",
       "       [46, 10, 79, 39, 37,  8,  8, 37, 36, 46, 76, 23, 32, 32, 55, 54,\n",
       "        37, 36, 82, 79, 78, 15, 10, 82, 79, 45, 37, 79,  6, 48, 76, 79,\n",
       "        78, 15, 51, 79, 36, 15, 51, 60, 79, 51, 60, 46, 79, 45, 15, 10,\n",
       "         8, 78],\n",
       "       [78, 79, 16, 46, 46, 75, 79, 21, 78, 79, 39, 10, 37, 49, 79, 65,\n",
       "        46, 59, 46, 10, 65, 32, 51, 46,  8,  8, 15, 48, 45, 74, 74, 65,\n",
       "         6,  8, 36,  6, 19, 78, 65,  9, 17, 32, 32, 55, 14, 39, 79, 47,\n",
       "        37, 21],\n",
       "       [ 6, 48, 76, 79, 46, 19, 46, 76, 79, 51, 60, 46, 32, 78, 51, 10,\n",
       "         6, 48, 45, 46, 10, 78, 79, 36, 15, 51, 60, 79,  6, 79, 47, 37,\n",
       "        48, 78, 21, 49, 15, 48, 45, 79, 47, 21, 10, 15, 37, 78, 15, 51,\n",
       "        19, 56],\n",
       "       [79, 51, 60, 46, 48, 79,  6, 48, 37, 51, 60, 46, 10, 79, 75,  6,\n",
       "        15, 10, 32, 37, 39, 79, 46, 19, 46, 78, 79, 39, 37,  8,  8, 37,\n",
       "        36, 46, 76, 79, 51, 60, 46, 79, 49, 15, 48, 15, 78, 51, 46, 10,\n",
       "        73, 78],\n",
       "       [51, 46, 49, 75,  8,  6, 51, 46, 79, 51, 60, 46, 79, 51, 36, 37,\n",
       "        79, 67, 46, 15, 48, 45, 78, 79, 75, 10, 46, 78, 46, 48, 51, 46,\n",
       "        76,  1, 17, 32, 32, 25, 60, 15, 78, 79, 48, 15, 45, 60, 51, 49,\n",
       "         6, 10],\n",
       "       [75, 37, 48, 79, 60, 15, 78, 79, 16, 48, 46, 46, 78, 74, 74, 78,\n",
       "        51, 15, 10, 10, 46, 76, 79, 60, 15, 49, 79, 21, 75, 79, 36, 15,\n",
       "        51, 60, 32, 60, 15, 78, 79, 39, 37, 37, 51, 79,  6, 48, 76, 79,\n",
       "        78,  6],\n",
       "       [76, 15, 10, 46, 47, 51, 15, 37, 48, 82, 79, 48, 37, 36, 56, 79,\n",
       "        51, 60, 46, 79, 51, 10, 46,  6, 78, 21, 10, 46, 79, 49, 21, 78,\n",
       "        51, 79, 67, 46, 79, 78, 51, 15,  8,  8, 32, 15, 48, 79, 54, 37,\n",
       "         1, 79],\n",
       "       [51, 60, 46, 79, 47, 10, 37, 78, 78, 65, 82, 73, 79, 60, 46, 19,\n",
       "         9, 79,  4, 15, 45, 60, 51, 79, 19, 37, 48, 76, 46, 10, 73, 78,\n",
       "        32, 36, 60, 46, 10, 46, 79,  5, 79, 78,  6, 36, 79,  5, 48, 57,\n",
       "        21, 48]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Check\n",
    "\n",
    "Remember this will take a lot longer on CPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the LSTM Model\n",
    "\n",
    "**Note! We will have options for GPU users and CPU users. CPU will take MUCH LONGER to train and you may encounter RAM issues depending on your hardware. If that is the case, consider using cloud services like AWS, GCP, or Azure. Note, these may cost you money to use!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=3,drop_prob=0.5,use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # SET UP ATTRIBUTES\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                  \n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users.\n",
    "        '''\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=128,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param  = []\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to make the total_parameters be roughly the same magnitude as the number of characters in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384596"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406270"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data to be used for training\n",
    "train_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406270"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40627"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Feel free to play around with these values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VARIABLES\n",
    "\n",
    "# Epochs to train for\n",
    "epochs = 5\n",
    "# batch size \n",
    "batch_size = 128\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "------\n",
    "\n",
    "## Saving the Model\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful to overwrite our original name file!\n",
    "model_name = 'example.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=128,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 128, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc_linear): Linear(in_features=128, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "        # Encode raw letters with model\n",
    "        encoded_text = model.encoder[char]\n",
    "        \n",
    "        # set as numpy array for one hot encoding\n",
    "        # NOTE THE [[ ]] dimensions!!\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        \n",
    "        # One hot encoding\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        # Check for CPU\n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        \n",
    "        # Grab hidden states\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        \n",
    "        # Run model and get predicted output\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "        \n",
    "        # Convert lstm_out to probabilities\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            # move back to CPU to use with numpy\n",
    "            probs = probs.cpu()\n",
    "        \n",
    "        \n",
    "        # k determines how many characters to consider\n",
    "        # for our probability choice.\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
    "        \n",
    "        # Return k largest probabilities in tensor\n",
    "        probs, index_positions = probs.topk(k)\n",
    "        \n",
    "        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        \n",
    "        # Create array of probabilities\n",
    "        probs = probs.numpy().flatten()\n",
    "        \n",
    "        # Convert to probabilities per index\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "        \n",
    "      \n",
    "    \n",
    "    # CHECK FOR GPU\n",
    "    if(model.use_gpu):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # begin output from initial seed\n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    # intiate hidden state\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    # predict the next character for every character in seed\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "    # add initial characters to output\n",
    "    output_chars.append(char)\n",
    "    \n",
    "    # Now generate for size requested\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West ee  e t    tt  t     e t   e   e tt  eett ettt       e et  t  t ttt e  ette te    tee  te   ee    e t t t  te t  e    e t  et eeee   t et ttee    e  te   ee e ee t  ee t      te ettee e ee eeee  tet e  te   t eee t   eee  te  t tt t  eet  e  tt eetee etet  t  e e    t   t e  e   tte et t t  t  ee  te   t t  eeee  e   eett  t t   te ett te e tt t   te eett     eeee  eee  t    tte e teee ttte eeet t    te e    t eete e e e      te e   e       e ee ee  ettt          te      et t t  ttte et te  tte  tt teet t etet ete     ee  e    tt   ee e  et  e  t    eettt  eete   tt ee e  et  et    t t e ett tt   t etet   te te  te eeeee t tee ee tt t   e   tt    e  eettt e tt et   tetee e e e ettet ttee   e e tee e  t t t  e e ee  t  eeeteeeet  e   e  e   e te    te eeee  e   tee tettteeeet ee   teet e  ee    ee teet t t e  te  t     t ee t  e    t e te     tee  e tet tt    eete e   e     ee etete  eee  t  t        t   tt  t    t  e t  t  ettt t tt   e te e t ee ttet  eeteee tte  tee e ee ete  t et    \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='West ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
